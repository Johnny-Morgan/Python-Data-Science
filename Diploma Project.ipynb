{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import random\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import movie_reviews, stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.append(\".\")\n",
    "stop_words.append(\",\")\n",
    "stop_words.append(\"-\")\n",
    "stop_words.append(\";\")\n",
    "stop_words.append(\"â€™\")\n",
    "stop_words.append(\"'s\")\n",
    "stop_words.append(\"%\")\n",
    "stop_words.append(\"n't\")\n",
    "stop_words.append(\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"https://www.taoiseach.gov.ie/eng/News/Taoiseach's_Speeches/New%20Year's%20Day%20Statement%20by%20An%20Taoiseach.html\")\n",
    "c = r.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(c, \"html.parser\")\n",
    "\n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = soup.find(\"div\", {\"class\":\"contentSub\"}).text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = all.replace(\"\\xa0\",\"\")\n",
    "all = all.replace(\"\\r\\n\",\"\")\n",
    "all = all.replace(\"var mapOverlayUrl = ''\",\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = all.find_all(\"div\", {\"dir\":\"ltr\"})[0].text\n",
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"C:/Users/Johnny/Desktop/Data Science/Project/speech1.txt\", \"w\") as out_file:\n",
    "#     out_file.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape links to speeches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = requests.get(\"https://www.taoiseach.gov.ie/eng/News/Taoiseach's_Speeches/\")\n",
    "c1 = r1.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(c1, \"html.parser\")\n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/eng/News/Taoiseach's_Speeches/New Year's Day Statement by An Taoiseach.html\n",
      "/eng/News/Taoiseach's_Speeches/Speech_by_An_Taoiseach_Official_opening_of_new_Barclays_Offices.html\n",
      "/eng/News/Taoiseach's_Speeches/Speech_of_An_Taoiseach_Disruptive_Technologies_Innovation_Fund_Monday_10_December_2018.html\n",
      "/eng/News/Taoiseach's_Speeches/Speech_of_An_Taoiseach_Leo_Varadkar_T_D_TEG_%E2%80%93_Jobs_Announcement_Mullingar_7_December_2018.html\n",
      "/eng/News/Taoiseach's_Speeches/Speech_by_An_Taoiseach_Launch_of_The_Sunday_Papers_Irish_Times_Building_Tara_Street_Tuesday_4_December_2018.html\n",
      "/eng/News/Taoiseach's_Speeches/Speech_of_An_Taoiseach_Votail_100_Reception_St_Patrick_s_Hall_Dublin_Castle.html\n",
      "/eng/News/Taoiseach's_Speeches/Speech_by_An_Taoiseach_at_the_Garda_Passing_out_Parade_Garda_College_Templemore_.html\n",
      "/eng/News/Taoiseach's_Speeches/Speech_by_An_Taoiseach_Announcement_of_First_Round_of_Funding_for_the_Climate_Action_Fund.html\n",
      "/eng/News/Taoiseach's_Speeches/Announcement_of_First_Round_of_Funding_for_the_Urban_Regeneration_and_Development_Fund.html\n",
      "/eng/News/Taoiseach's_Speeches/Speech_by_An_Taoiseach_Leo_Varadkar_T_D_Announcement_of_First_Round_of_Funding_for_the_Rural_Regeneration_and_Development_Fund_Friday_23_November_2018.html\n"
     ]
    }
   ],
   "source": [
    "all = soup.find_all(\"span\",{\"class\":\"ItemName\"})\n",
    "for a in all:\n",
    "    print(a.find(\"a\")[\"href\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2013 - 2019 Speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.taoiseach.gov.ie/eng/News/Taoiseach's_Speeches/?pageNumber=\"\n",
    "speech_string_2018 = \"\"\n",
    "speech_string_2017 = \"\"\n",
    "speech_string_2016 = \"\"\n",
    "speech_string_2015 = \"\"\n",
    "speech_string_2014 = \"\"\n",
    "speech_string_2013 = \"\"\n",
    "\n",
    "leo_string = \"\"\n",
    "enda_string = \"\"\n",
    "brian_string = \"\"\n",
    "bertie_string = \"\"\n",
    "\n",
    "count = 1\n",
    "\n",
    "for page in range(1,38):\n",
    "    r = requests.get(base_url + str(page))\n",
    "    c = r.content\n",
    "    soup = BeautifulSoup(c, \"html.parser\")\n",
    "    all = soup.find_all(\"span\",{\"class\":\"ItemName\"})\n",
    "    for a in all:\n",
    "        r = requests.get(\"https://www.taoiseach.gov.ie\" + a.find(\"a\")[\"href\"])\n",
    "        c = r.content\n",
    "        \n",
    "        soup = BeautifulSoup(c, \"html.parser\")\n",
    "        all = soup.find(\"div\", {\"class\":\"contentSub\"}).text\n",
    "        all = all.replace(\"var mapOverlayUrl = '';\",\"\")\n",
    "        \n",
    "        if count < 54:\n",
    "            #with open(\"C:/Users/Johnny/Desktop/Data Science/Project/speeches/2018/2018_speech_\" + str(54 - count) + \".txt\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "                #out_file.write(all)\n",
    "            speech_string_2018 += all\n",
    "        elif count < 127:\n",
    "            #with open(\"C:/Users/Johnny/Desktop/Data Science/Project/speeches/2017/2017_speech_\" + str(127 - count) + \".txt\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "                #out_file.write(all)\n",
    "            speech_string_2017 += all\n",
    "        elif count < 204:\n",
    "            #with open(\"C:/Users/Johnny/Desktop/Data Science/Project/speeches/2016/2016_speech_\" + str(204 - count) + \".txt\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "                #out_file.write(all)\n",
    "            speech_string_2016 += all\n",
    "        elif count < 304:\n",
    "            #with open(\"C:/Users/Johnny/Desktop/Data Science/Project/speeches/2015/2015_speech_\" + str(304 - count) + \".txt\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "                #out_file.write(all)\n",
    "            speech_string_2015 += all\n",
    "        elif count < 366:\n",
    "            #with open(\"C:/Users/Johnny/Desktop/Data Science/Project/speeches/2014/2014_speech_\" + str(366 - count) + \".txt\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "                #out_file.write(all)\n",
    "            speech_string_2014 += all\n",
    "        elif count < 370:\n",
    "            #with open(\"C:/Users/Johnny/Desktop/Data Science/Project/speeches/2013/2013_speech_\" + str(370 - count) + \".txt\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "                #out_file.write(all)\n",
    "            speech_string_2013 += all\n",
    "        count = count + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2018\n",
    "count = 53\n",
    "speech_string_2018 = speech_string_2018.lower()\n",
    "words_2018 = word_tokenize(speech_string_2018)\n",
    "\n",
    "print(\"There were %d speeches in 2018 with %d words with an average of %d per speech\" % (count, len(words_2018), len(words_2018)/(count)))\n",
    "print()\n",
    "\n",
    "words_2018 = [w for w in words_2018 if not w in stop_words]\n",
    "all_words_2018 = nltk.FreqDist(words_2018)\n",
    "\n",
    "print(\"There were %d speeches in 2018 with %d words with an average of %d per speech\" % (count, len(words_2018), len(words_2018)/(count)))\n",
    "print()\n",
    "\n",
    "print(all_words_2018.most_common(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2017\n",
    "count = 73\n",
    "speech_string_2017 = speech_string_2017.lower()\n",
    "words_2017 = word_tokenize(speech_string_2017)\n",
    "\n",
    "print(\"There were %d speeches in 2017 with %d words with an average of %d per speech\" % (count, len(words_2017), len(words_2017)/(count)))\n",
    "print()\n",
    "\n",
    "words_2017 = [w for w in words_2017 if not w in stop_words]\n",
    "all_words_2017 = nltk.FreqDist(words_2017)\n",
    "\n",
    "print(\"There were %d speeches in 2017 with %d words with an average of %d per speech\" % (count, len(words_2017), len(words_2017)/(count)))\n",
    "print()\n",
    "\n",
    "print(all_words_2017.most_common(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2016\n",
    "count = 77\n",
    "speech_string_2016 = speech_string_2016.lower()\n",
    "words_2016 = word_tokenize(speech_string_2016)\n",
    "\n",
    "print(\"There were %d speeches in 2016 with %d words with an average of %d per speech\" % (count, len(words_2016), len(words_2016)/(count)))\n",
    "print()\n",
    "\n",
    "words_2016 = [w for w in words_2016 if not w in stop_words]\n",
    "all_words_2016 = nltk.FreqDist(words_2016)\n",
    "\n",
    "print(\"There were %d speeches in 2016 with %d words with an average of %d per speech\" % (count, len(words_2016), len(words_2016)/(count)))\n",
    "print()\n",
    "\n",
    "print(all_words_2016.most_common(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2015\n",
    "count = 100\n",
    "speech_string_2015 = speech_string_2015.lower()\n",
    "words_2015 = word_tokenize(speech_string_2015)\n",
    "\n",
    "print(\"There were %d speeches in 2015 with %d words with an average of %d per speech\" % (count, len(words_2015), len(words_2015)/(count)))\n",
    "print()\n",
    "\n",
    "words_2015 = [w for w in words_2015 if not w in stop_words]\n",
    "all_words_2015 = nltk.FreqDist(words_2015)\n",
    "\n",
    "print(\"There were %d speeches in 2015 with %d words with an average of %d per speech\" % (count, len(words_2015), len(words_2015)/(count)))\n",
    "print()\n",
    "\n",
    "print(all_words_2015.most_common(100))\n",
    "\n",
    "print(\"The word Brexit occurs \" + str(all_words_2015[\"brexit\"]) + \" times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2014\n",
    "count = 62\n",
    "speech_string_2014 = speech_string_2014.lower()\n",
    "words_2014 = word_tokenize(speech_string_2014)\n",
    "\n",
    "print(\"There were %d speeches in 2014 with %d words with an average of %d per speech\" % (count, len(words_2014), len(words_2014)/(count)))\n",
    "print()\n",
    "\n",
    "words_2014 = [w for w in words_2014 if not w in stop_words]\n",
    "all_words_2014 = nltk.FreqDist(words_2014)\n",
    "\n",
    "print(\"There were %d speeches in 2014 with %d words with an average of %d per speech\" % (count, len(words_2014), len(words_2014)/(count)))\n",
    "print()\n",
    "\n",
    "print(all_words_2014.most_common(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2013\n",
    "# count = 3\n",
    "# speech_string_2013 = speech_string_2013.lower()\n",
    "# words_2013 = word_tokenize(speech_string_2013)\n",
    "\n",
    "# print(\"There were %d speeches in 2013 with %d words with an average of %d per speech\" % (count, len(words_2013), len(words_2013)/(count)))\n",
    "# print()\n",
    "\n",
    "# words_2013 = [w for w in words_2013 if not w in stop_words]\n",
    "# all_words_2013 = nltk.FreqDist(words_2013)\n",
    "\n",
    "# print(\"There were %d speeches in 2013 with %d words with an average of %d per speech\" % (count, len(words_2013), len(words_2013)/(count)))\n",
    "# print()\n",
    "\n",
    "# print(all_words_2013.most_common(100))\n",
    "# print()\n",
    "# print(all_words_2013[\"abortion\"])\n",
    "# print(all_words_2013[\"recession\"])\n",
    "# recession_2013 = all_words_2013[\"recession\"]/len(words_2013)*100\n",
    "# print(recession_2013)\n",
    "# print(all_words_2013[\"bailout\"])\n",
    "# print()\n",
    "# print(all_words_2013[\"brexit\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2013-19 combined\n",
    "# count = 368\n",
    "# speech_string_2013_19 = speech_string_2013_19.lower()\n",
    "# words_2013_19 = word_tokenize(speech_string_2013_19)\n",
    "\n",
    "# print(\"There were %d speeches in 2013-19 with %d words with an average of %d per speech\" % (count, len(words_2013_19), len(words_2013_19)/(count)))\n",
    "# print()\n",
    "\n",
    "# words_2013_19 = [w for w in words_2013_19 if not w in stop_words]\n",
    "# all_words_2013_19 = nltk.FreqDist(words_2013_19)\n",
    "\n",
    "# print(\"There were %d speeches in 2013-19 with %d words with an average of %d per speech\" % (count, len(words_2013_19), len(words_2013_19)/(count)))\n",
    "# print()\n",
    "\n",
    "# print(all_words_2013_19.most_common(100))\n",
    "# print(all_words_2013_19[\"tiger\"])\n",
    "# print(all_words_2013_19[\"brexit\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2012 Speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.taoiseach.gov.ie/eng/News/Archives/2012/Taoiseach's_Speeches_2012?pageNumber=\"\n",
    "speech_string_2012 = \"\"\n",
    "\n",
    "count = 1\n",
    "for page in range(1,6):\n",
    "    r = requests.get(base_url + str(page))\n",
    "    c = r.content\n",
    "    soup = BeautifulSoup(c, \"html.parser\")\n",
    "    all = soup.find_all(\"span\",{\"class\":\"ItemName\"})\n",
    "    for a in all:\n",
    "        r = requests.get(\"https://www.taoiseach.gov.ie\" + a.find(\"a\")[\"href\"])\n",
    "        c = r.content\n",
    "        \n",
    "        soup = BeautifulSoup(c, \"html.parser\")\n",
    "        all = soup.find(\"div\", {\"class\":\"contentSub\"}).text\n",
    "        all = all.replace(\"var mapOverlayUrl = '';\",\"\")\n",
    "        \n",
    "#         with open(\"C:/Users/Johnny/Desktop/Data Science/Project/speeches/2012/2012_speech_\" + str(count) + \".txt\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "#             out_file.write(all)\n",
    "        count = count + 1\n",
    "        speech_string_2012 += all\n",
    "        item = (word_tokenize(all), \"fg\")\n",
    "        speech_list.append(item)\n",
    "        \n",
    "#print(speech_string_2012)\n",
    "print(str(count - 1) + \" speeches in 2012\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 44\n",
    "speech_string_2012 = speech_string_2012.lower()\n",
    "words_2012 = word_tokenize(speech_string_2012)\n",
    "\n",
    "print(\"There were %d speeches in 2012 with %d words with an average of %d per speech\" % (count, len(words_2012), len(words_2012)/(count)))\n",
    "print()\n",
    "\n",
    "words_2012 = [w for w in words_2012 if not w in stop_words]\n",
    "all_words_2012 = nltk.FreqDist(words_2012)\n",
    "\n",
    "print(\"There were %d speeches in 2012 with %d words with an average of %d per speech\" % (count, len(words_2012), len(words_2012)/(count)))\n",
    "print()\n",
    "\n",
    "print(all_words_2012.most_common(100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2011 Speeches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.taoiseach.gov.ie/eng/News/Archives/2011/Taoiseach's_Speeches_2011?pageNumber=\"\n",
    "speech_string_2011 = \"\"\n",
    "\n",
    "count = 1\n",
    "for page in range(1,5):\n",
    "    r = requests.get(base_url + str(page))\n",
    "    c = r.content\n",
    "    soup = BeautifulSoup(c, \"html.parser\")\n",
    "    all = soup.find_all(\"span\",{\"class\":\"ItemName\"})\n",
    "    for a in all:\n",
    "        r = requests.get(\"https://www.taoiseach.gov.ie\" + a.find(\"a\")[\"href\"])\n",
    "        c = r.content\n",
    "        \n",
    "        soup = BeautifulSoup(c, \"html.parser\")\n",
    "        all = soup.find(\"div\", {\"class\":\"contentSub\"}).text\n",
    "        all = all.replace(\"var mapOverlayUrl = '';\",\"\")\n",
    "        \n",
    "#         with open(\"C:/Users/Johnny/Desktop/Data Science/Project/speeches/2011/2011_speech_\" + str(count) + \".txt\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "#             out_file.write(all)\n",
    "        count = count + 1\n",
    "        speech_string_2011 += all\n",
    "        item = (word_tokenize(all), \"fg\")\n",
    "        speech_list.append(item)\n",
    "        \n",
    "#print(speech_string_2011)\n",
    "print(str(count - 1) + \" speeches in 2011\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 38\n",
    "speech_string_2011 = speech_string_2011.lower()\n",
    "words_2011 = word_tokenize(speech_string_2011)\n",
    "\n",
    "print(\"There were %d speeches in 2011 with %d words with an average of %d per speech\" % (count, len(words_2011), len(words_2011)/(count)))\n",
    "print()\n",
    "\n",
    "words_2011 = [w for w in words_2011 if not w in stop_words]\n",
    "all_words_2011 = nltk.FreqDist(words_2011)\n",
    "\n",
    "print(\"There were %d speeches in 2011 with %d words with an average of %d per speech\" % (count, len(words_2011), len(words_2011)/(count)))\n",
    "print()\n",
    "\n",
    "print(all_words_2011.most_common(100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2010 Speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.taoiseach.gov.ie/eng/News/Archives/2010/Taoiseach's_Speeches_2010?pageNumber=\"\n",
    "speech_string_2010 = \"\"\n",
    "\n",
    "count = 1\n",
    "for page in range(1,6):\n",
    "    r = requests.get(base_url + str(page))\n",
    "    c = r.content\n",
    "    soup = BeautifulSoup(c, \"html.parser\")\n",
    "    all = soup.find_all(\"span\",{\"class\":\"ItemName\"})\n",
    "    for a in all:\n",
    "        r = requests.get(\"https://www.taoiseach.gov.ie\" + a.find(\"a\")[\"href\"])\n",
    "        c = r.content\n",
    "        \n",
    "        soup = BeautifulSoup(c, \"html.parser\")\n",
    "        all = soup.find(\"div\", {\"class\":\"contentSub\"}).text\n",
    "        all = all.replace(\"var mapOverlayUrl = '';\",\"\")\n",
    "        \n",
    "#         with open(\"C:/Users/Johnny/Desktop/Data Science/Project/speeches/2010/2010_speech_\" + str(count) + \".txt\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "#             out_file.write(all)\n",
    "        count = count + 1\n",
    "        speech_string_2010 += all\n",
    "        item = (word_tokenize(all), \"ff\")\n",
    "        speech_list.append(item)\n",
    "        \n",
    "#print(speech_string_2010)\n",
    "print(str(count - 1) + \" speeches in 2010\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 46\n",
    "speech_string_2010 = speech_string_2010.lower()\n",
    "words_2010 = word_tokenize(speech_string_2010)\n",
    "\n",
    "print(\"There were %d speeches in 2010 with %d words with an average of %d per speech\" % (count, len(words_2010), len(words_2010)/(count)))\n",
    "print()\n",
    "\n",
    "words_2010 = [w for w in words_2010 if not w in stop_words]\n",
    "all_words_2010 = nltk.FreqDist(words_2010)\n",
    "\n",
    "print(\"There were %d speeches in 2010 with %d words with an average of %d per speech\" % (count, len(words_2010), len(words_2010)/(count)))\n",
    "print()\n",
    "\n",
    "print(all_words_2010.most_common(100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2009 Speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.taoiseach.gov.ie/eng/News/Archives/2009/Taoiseach's_Speeches_2009?pageNumber=\"\n",
    "speech_string_2009 = \"\"\n",
    "\n",
    "count = 1\n",
    "for page in range(1,6):\n",
    "    r = requests.get(base_url + str(page))\n",
    "    c = r.content\n",
    "    soup = BeautifulSoup(c, \"html.parser\")\n",
    "    all = soup.find_all(\"span\",{\"class\":\"ItemName\"})\n",
    "    for a in all:\n",
    "        r = requests.get(\"https://www.taoiseach.gov.ie\" + a.find(\"a\")[\"href\"])\n",
    "        c = r.content\n",
    "        \n",
    "        soup = BeautifulSoup(c, \"html.parser\")\n",
    "        all = soup.find(\"div\", {\"class\":\"contentSub\"}).text\n",
    "        all = all.replace(\"var mapOverlayUrl = '';\",\"\")\n",
    "        \n",
    "#         with open(\"C:/Users/Johnny/Desktop/Data Science/Project/speeches/2009/2009_speech_\" + str(count) + \".txt\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "#             out_file.write(all)\n",
    "        count = count + 1\n",
    "        speech_string_2009 += all\n",
    "        item = (word_tokenize(all), \"ff\")\n",
    "        speech_list.append(item)\n",
    "        \n",
    "        \n",
    "#print(speech_string_2009)\n",
    "print(str(count - 1) + \" speeches in 2009\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 41\n",
    "speech_string_2009 = speech_string_2009.lower()\n",
    "words_2009 = word_tokenize(speech_string_2009)\n",
    "\n",
    "print(\"There were %d speeches in 2009 with %d words with an average of %d per speech\" % (count, len(words_2009), len(words_2009)/(count)))\n",
    "print()\n",
    "\n",
    "words_2009 = [w for w in words_2009 if not w in stop_words]\n",
    "all_words_2009 = nltk.FreqDist(words_2009)\n",
    "\n",
    "print(\"There were %d speeches in 2009 with %d words with an average of %d per speech\" % (count, len(words_2009), len(words_2009)/(count)))\n",
    "print()\n",
    "\n",
    "print(all_words_2009.most_common(100))\n",
    "\n",
    "print(all_words_2009[\"recession\"])\n",
    "recession_2009 = all_words_2009[\"recession\"]/len(words_2009)*100\n",
    "print(recession_2009)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2008 Speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_url = \"https://www.taoiseach.gov.ie/eng/News/Archives/2008/Taoiseach's_Speeches_2008?pageNumber=\"\n",
    "# speech_string_2008 = \"\"\n",
    "\n",
    "# count = 1\n",
    "# for page in range(1,6):\n",
    "#     r = requests.get(base_url + str(page))\n",
    "#     c = r.content\n",
    "#     soup = BeautifulSoup(c, \"html.parser\")\n",
    "#     all = soup.find_all(\"span\",{\"class\":\"ItemName\"})\n",
    "#     for a in all:\n",
    "#         r = requests.get(\"https://www.taoiseach.gov.ie\" + a.find(\"a\")[\"href\"])\n",
    "#         c = r.content\n",
    "        \n",
    "#         soup = BeautifulSoup(c, \"html.parser\")\n",
    "#         all = soup.find(\"div\", {\"class\":\"contentSub\"}).text\n",
    "#         all = all.replace(\"var mapOverlayUrl = '';\",\"\")\n",
    "        \n",
    "# #         with open(\"C:/Users/Johnny/Desktop/Data Science/Project/speeches/2008/2008_speech_\" + str(count) + \".txt\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "# #             out_file.write(all)\n",
    "#         count = count + 1\n",
    "        \n",
    "#         speech_string_2008 += all\n",
    "        \n",
    "# #print(speech_string_2008)\n",
    "# print(str(count - 1) + \" speeches in 2008\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 2\n",
    "# speech_string_2008 = speech_string_2008.lower()\n",
    "# words_2008 = word_tokenize(speech_string_2008)\n",
    "\n",
    "# print(\"There were %d speeches in 2008 with %d words with an average of %d per speech\" % (count, len(words_2008), len(words_2008)/(count)))\n",
    "# print()\n",
    "\n",
    "# words_2008 = [w for w in words_2008 if not w in stop_words]\n",
    "# all_words_2008 = nltk.FreqDist(words_2008)\n",
    "\n",
    "# print(\"There were %d speeches in 2008 with %d words with an average of %d per speech\" % (count, len(words_2008), len(words_2008)/(count)))\n",
    "# print()\n",
    "\n",
    "# print(all_words_2008.most_common(100))\n",
    "# print(all_words_2008[\"tiger\"])\n",
    "# print(all_words_2008[\"celtic\"])\n",
    "# print()\n",
    "# print(all_words_2008[\"abortion\"])\n",
    "# print(all_words_2008[\"recession\"])\n",
    "# print(all_words_2008[\"bailout\"])\n",
    "# print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2007 & 2006 unavailable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2005 Speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.taoiseach.gov.ie/eng/News/Archives/2005/Taoiseach's_Speeches_Archive_2005/?pageNumber=\"\n",
    "speech_string_2005 = \"\"\n",
    "\n",
    "count = 1\n",
    "for page in range(1,17):\n",
    "    r = requests.get(base_url + str(page))\n",
    "    c = r.content\n",
    "    soup = BeautifulSoup(c, \"html.parser\")\n",
    "    all = soup.find_all(\"span\",{\"class\":\"ItemName\"})\n",
    "    for a in all:\n",
    "        r = requests.get(\"https://www.taoiseach.gov.ie\" + a.find(\"a\")[\"href\"])\n",
    "        c = r.content\n",
    "        \n",
    "        soup = BeautifulSoup(c, \"html.parser\")\n",
    "        all = soup.find(\"div\", {\"class\":\"contentSub\"}).text\n",
    "        all = all.replace(\"var mapOverlayUrl = '';\",\"\")\n",
    "        \n",
    "#         with open(\"C:/Users/Johnny/Desktop/Data Science/Project/speeches/2005/2005_speech_\" + str(count) + \".txt\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "#             out_file.write(all)\n",
    "        count = count + 1\n",
    "        speech_string_2005 += all\n",
    "        item = (word_tokenize(all), \"ff\")\n",
    "        speech_list.append(item)\n",
    "        \n",
    "        \n",
    "#print(speech_string_2005)\n",
    "print(str(count - 1) + \" speeches in 2005\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 152\n",
    "speech_string_2005 = speech_string_2005.lower()\n",
    "words_2005 = word_tokenize(speech_string_2005)\n",
    "\n",
    "print(\"There were %d speeches in 2005 with %d words with an average of %d per speech\" % (count, len(words_2005), len(words_2005)/(count)))\n",
    "print()\n",
    "\n",
    "words_2005 = [w for w in words_2005 if not w in stop_words]\n",
    "all_words_2005 = nltk.FreqDist(words_2005)\n",
    "print(\"There were %d speeches in 2005 with %d words with an average of %d per speech\" % (count, len(words_2005), len(words_2005)/(count)))\n",
    "print()\n",
    "\n",
    "print(all_words_2005.most_common(100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2004 Speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.taoiseach.gov.ie/eng/News/Archives/2004/Taoiseach's_Speeches_Archive_2004/?pageNumber=\"\n",
    "speech_string_2004 = \"\"\n",
    "\n",
    "count = 1\n",
    "for page in range(1,4):\n",
    "    r = requests.get(base_url + str(page))\n",
    "    c = r.content\n",
    "    soup = BeautifulSoup(c, \"html.parser\")\n",
    "    all = soup.find_all(\"span\",{\"class\":\"ItemName\"})\n",
    "    for a in all:\n",
    "        r = requests.get(\"https://www.taoiseach.gov.ie\" + a.find(\"a\")[\"href\"])\n",
    "        c = r.content\n",
    "        \n",
    "        soup = BeautifulSoup(c, \"html.parser\")\n",
    "        all = soup.find(\"div\", {\"class\":\"contentSub\"}).text\n",
    "        all = all.replace(\"var mapOverlayUrl = '';\",\"\")\n",
    "        \n",
    "#         with open(\"C:/Users/Johnny/Desktop/Data Science/Project/speeches/2004/2004_speech_\" + str(count) + \".txt\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "#             out_file.write(all)\n",
    "        count = count + 1\n",
    "        \n",
    "        speech_string_2004 += all\n",
    "        item = (word_tokenize(all), \"ff\")\n",
    "        speech_list.append(item)\n",
    "        \n",
    "        \n",
    "#print(speech_string_2004)\n",
    "print(str(count - 1) + \" speeches in 2004\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 30\n",
    "speech_string_2004 = speech_string_2004.lower()\n",
    "words_2004 = word_tokenize(speech_string_2004)\n",
    "\n",
    "print(\"There were %d speeches in 2004 with %d words with an average of %d per speech\" % (count, len(words_2004), len(words_2004)/(count)))\n",
    "print()\n",
    "\n",
    "words_2004 = [w for w in words_2004 if not w in stop_words]\n",
    "all_words_2004 = nltk.FreqDist(words_2004)\n",
    "\n",
    "print(\"There were %d speeches in 2004 with %d words with an average of %d per speech\" % (count, len(words_2004), len(words_2004)/(count)))\n",
    "print()\n",
    "\n",
    "print(all_words_2004.most_common(100))\n",
    "\n",
    "print(all_words_2004[\"recession\"])\n",
    "recession_2004 = all_words_2004[\"recession\"]/len(words_2004)*100\n",
    "print(recession_2004)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2003 Speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.taoiseach.gov.ie/eng/News/Archives/2003/Taoiseach's_Speeches_Archive_2003/?pageNumber=\"\n",
    "speech_string_2003 = \"\"\n",
    "\n",
    "count = 1\n",
    "for page in range(1,5):\n",
    "    r = requests.get(base_url + str(page))\n",
    "    c = r.content\n",
    "    soup = BeautifulSoup(c, \"html.parser\")\n",
    "    all = soup.find_all(\"span\",{\"class\":\"ItemName\"})\n",
    "    for a in all:\n",
    "        try:\n",
    "            r = requests.get(\"https://www.taoiseach.gov.ie\" + a.find(\"a\")[\"href\"])\n",
    "            c = r.content\n",
    "\n",
    "            soup = BeautifulSoup(c, \"html.parser\")\n",
    "            all = soup.find(\"div\", {\"class\":\"contentSub\"}).text\n",
    "            all = all.replace(\"var mapOverlayUrl = '';\",\"\")\n",
    "\n",
    "#             with open(\"C:/Users/Johnny/Desktop/Data Science/Project/speeches/2003/2003_speech_\" + str(count) + \".txt\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "#                 out_file.write(all)\n",
    "            count = count + 1\n",
    "            speech_string_2003 += all\n",
    "            item = (word_tokenize(all), \"ff\")\n",
    "            speech_list.append(item)\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "#print(speech_string_2003)\n",
    "print(str(count - 1) + \" speeches in 2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count =34\n",
    "speech_string_2003 = speech_string_2003.lower()\n",
    "words_2003 = word_tokenize(speech_string_2003)\n",
    "\n",
    "print(\"There were %d speeches in 2003 with %d words with an average of %d per speech\" % (count, len(words_2003), len(words_2003)/(count)))\n",
    "print()\n",
    "\n",
    "words_2003 = [w for w in words_2003 if not w in stop_words]\n",
    "all_words_2003 = nltk.FreqDist(words_2003)\n",
    "\n",
    "print(\"There were %d speeches in 2003 with %d words with an average of %d per speech\" % (count, len(words_2003), len(words_2003)/(count)))\n",
    "print()\n",
    "\n",
    "print(all_words_2003.most_common(100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2002 Speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.taoiseach.gov.ie/eng/News/Archives/2002/Taoiseach's_Speeches_Archive_2002/?pageNumber=\"\n",
    "speech_string_2002 = \"\"\n",
    "\n",
    "count = 1\n",
    "for page in range(1,9):\n",
    "    r = requests.get(base_url + str(page))\n",
    "    c = r.content\n",
    "    soup = BeautifulSoup(c, \"html.parser\")\n",
    "    all = soup.find_all(\"span\",{\"class\":\"ItemName\"})\n",
    "    for a in all:\n",
    "        try:\n",
    "            r = requests.get(\"https://www.taoiseach.gov.ie\" + a.find(\"a\")[\"href\"])\n",
    "            c = r.content\n",
    "\n",
    "            soup = BeautifulSoup(c, \"html.parser\")\n",
    "            all = soup.find(\"div\", {\"class\":\"contentSub\"}).text\n",
    "            all = all.replace(\"var mapOverlayUrl = '';\",\"\")\n",
    "\n",
    "#             with open(\"C:/Users/Johnny/Desktop/Data Science/Project/speeches/2002/2002_speech_\" + str(count) + \".txt\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "#                 out_file.write(all)\n",
    "            count = count + 1\n",
    "            speech_string_2002 += all\n",
    "            item = (word_tokenize(all), \"ff\")\n",
    "            speech_list.append(item)\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "#print(speech_string_2002)\n",
    "print(str(count - 1) + \" speeches in 2002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 73\n",
    "speech_string_2002 = speech_string_2002.lower()\n",
    "words_2002 = word_tokenize(speech_string_2002)\n",
    "\n",
    "print(\"There were %d speeches in 2002 with %d words with an average of %d per speech\" % (count, len(words_2002), len(words_2002)/(count)))\n",
    "print()\n",
    "\n",
    "words_2002 = [w for w in words_2002 if not w in stop_words]\n",
    "all_words_2002 = nltk.FreqDist(words_2002)\n",
    "\n",
    "print(\"There were %d speeches in 2002 with %d words with an average of %d per speech\" % (count, len(words_2002), len(words_2002)/(count)))\n",
    "print()\n",
    "\n",
    "print(all_words_2002.most_common(100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2001 Speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.taoiseach.gov.ie/eng/News/Archives/2001/Taoiseach's_Speeches_Archive_2001/?pageNumber=\"\n",
    "speech_string_2001 = \"\"\n",
    "\n",
    "count = 1\n",
    "for page in range(1,8):\n",
    "    r = requests.get(base_url + str(page))\n",
    "    c = r.content\n",
    "    soup = BeautifulSoup(c, \"html.parser\")\n",
    "    all = soup.find_all(\"span\",{\"class\":\"ItemName\"})\n",
    "    for a in all:\n",
    "        try:\n",
    "            r = requests.get(\"https://www.taoiseach.gov.ie\" + a.find(\"a\")[\"href\"])\n",
    "            c = r.content\n",
    "\n",
    "            soup = BeautifulSoup(c, \"html.parser\")\n",
    "            all = soup.find(\"div\", {\"class\":\"contentSub\"}).text\n",
    "            all = all.replace(\"var mapOverlayUrl = '';\",\"\")\n",
    "\n",
    "#             with open(\"C:/Users/Johnny/Desktop/Data Science/Project/speeches/2001/2001_speech_\" + str(count) + \".txt\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "#                 out_file.write(all)\n",
    "            count = count + 1\n",
    "            \n",
    "            speech_string_2001 += all\n",
    "            item = (word_tokenize(all), \"ff\")\n",
    "            speech_list.append(item)\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "#print(speech_string_2001)\n",
    "print(str(count - 1) + \" speeches in 2001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 65\n",
    "speech_string_2001 = speech_string_2001.lower()\n",
    "words_2001 = word_tokenize(speech_string_2001)\n",
    "\n",
    "print(\"There were %d speeches in 2001 with %d words with an average of %d per speech\" % (count, len(words_2001), len(words_2001)/(count)))\n",
    "print()\n",
    "\n",
    "words_2001 = [w for w in words_2001 if not w in stop_words]\n",
    "all_words_2001 = nltk.FreqDist(words_2001)\n",
    "\n",
    "print(\"There were %d speeches in 2001 with %d words with an average of %d per speech\" % (count, len(words_2001), len(words_2001)/(count)))\n",
    "print()\n",
    "\n",
    "print(all_words_2001.most_common(100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2000 Speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.taoiseach.gov.ie/eng/News/Archives/2000/Taoiseach's_Speeches_Archive_2000/?pageNumber=\"\n",
    "speech_string_2000 = \"\"\n",
    "\n",
    "count = 1\n",
    "for page in range(1,9):\n",
    "    r = requests.get(base_url + str(page))\n",
    "    c = r.content\n",
    "    soup = BeautifulSoup(c, \"html.parser\")\n",
    "    all = soup.find_all(\"span\",{\"class\":\"ItemName\"})\n",
    "    for a in all:\n",
    "        try:\n",
    "            r = requests.get(\"https://www.taoiseach.gov.ie\" + a.find(\"a\")[\"href\"])\n",
    "            c = r.content\n",
    "\n",
    "            soup = BeautifulSoup(c, \"html.parser\")\n",
    "            all = soup.find(\"div\", {\"class\":\"contentSub\"}).text\n",
    "            all = all.replace(\"var mapOverlayUrl = '';\",\"\")\n",
    "\n",
    "#             with open(\"C:/Users/Johnny/Desktop/Data Science/Project/speeches/2000/2000_speech_\" + str(count) + \".txt\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "#                 out_file.write(all)\n",
    "            count = count + 1\n",
    "            \n",
    "            speech_string_2000 += all\n",
    "            item = (word_tokenize(all), \"ff\")\n",
    "            speech_list.append(item)\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "#print(speech_string_2000)\n",
    "print(str(count - 1) + \" speeches in 2000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 73\n",
    "speech_string_2000 = speech_string_2000.lower()\n",
    "words_2000 = word_tokenize(speech_string_2000)\n",
    "\n",
    "print(\"There were %d speeches in 2000 with %d words with an average of %d per speech\" % (count, len(words_2000), len(words_2000)/(count)))\n",
    "print()\n",
    "\n",
    "words_2000 = [w for w in words_2000 if not w in stop_words]\n",
    "all_words_2000 = nltk.FreqDist(words_2000)\n",
    "\n",
    "print(\"There were %d speeches in 2000 with %d words with an average of %d per speech\" % (count, len(words_2000), len(words_2000)/(count)))\n",
    "print()\n",
    "\n",
    "print(all_words_2000.most_common(50))\n",
    "\n",
    "print(all_words_2000[\"recession\"])\n",
    "recession_2000 = all_words_2000[\"recession\"]/len(words_2000)*100\n",
    "print(recession_2000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1999 Speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.taoiseach.gov.ie/eng/News/Archives/1999/Taoiseach's_Speeches_Archive_1999/?pageNumber=\"\n",
    "speech_string_1999 = \"\"\n",
    "\n",
    "count = 1\n",
    "for page in range(1,9):\n",
    "    r = requests.get(base_url + str(page))\n",
    "    c = r.content\n",
    "    soup = BeautifulSoup(c, \"html.parser\")\n",
    "    all = soup.find_all(\"span\",{\"class\":\"ItemName\"})\n",
    "    for a in all:\n",
    "        try:\n",
    "            r = requests.get(\"https://www.taoiseach.gov.ie\" + a.find(\"a\")[\"href\"])\n",
    "            c = r.content\n",
    "\n",
    "            soup = BeautifulSoup(c, \"html.parser\")\n",
    "            all = soup.find(\"div\", {\"class\":\"contentSub\"}).text\n",
    "            all = all.replace(\"var mapOverlayUrl = '';\",\"\")\n",
    "\n",
    "            # Write each speech to a file\n",
    "#             with open(\"C:/Users/Johnny/Desktop/Data Science/Project/speeches/1999/1999_speech_\" + str(count) + \".txt\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "#                 out_file.write(all)\n",
    "            count = count + 1\n",
    "            \n",
    "            # create one sinlge string contain all speeches\n",
    "            speech_string_1999 += all\n",
    "            item = (word_tokenize(all), \"ff\")\n",
    "            speech_list.append(item)\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "#print(speech_string_1999)\n",
    "print(str(count - 1) + \" speeches in 1999\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 77\n",
    "speech_string_1999 = speech_string_1999.lower()\n",
    "words_1999 = word_tokenize(speech_string_1999)\n",
    "\n",
    "print(\"There were %d speeches in 1999 with %d words with an average of %d per speech\" % (count, len(words_1999), len(words_1999)/(count)))\n",
    "print()\n",
    "\n",
    "words_1999 = [w for w in words_1999 if not w in stop_words]\n",
    "all_words_1999 = nltk.FreqDist(words_1999)\n",
    "\n",
    "print(\"There were %d speeches in 1999 with %d words with an average of %d per speech\" % (count, len(words_1999), len(words_1999)/(count)))\n",
    "print()\n",
    "\n",
    "print(all_words_1999.most_common(50))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1998 Speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.taoiseach.gov.ie/eng/News/Archives/1998/Taoiseach's_Speeches_Archive_1998/?pageNumber=\"\n",
    "speech_string_1998 = \"\"\n",
    "\n",
    "count = 1\n",
    "for page in range(1,9):\n",
    "    r = requests.get(base_url + str(page))\n",
    "    c = r.content\n",
    "    soup = BeautifulSoup(c, \"html.parser\")\n",
    "    all = soup.find_all(\"span\",{\"class\":\"ItemName\"})\n",
    "    for a in all:\n",
    "        try:\n",
    "            r = requests.get(\"https://www.taoiseach.gov.ie\" + a.find(\"a\")[\"href\"])\n",
    "            c = r.content\n",
    "\n",
    "            soup = BeautifulSoup(c, \"html.parser\")\n",
    "            all = soup.find(\"div\", {\"class\":\"contentSub\"}).text\n",
    "            all = all.replace(\"var mapOverlayUrl = '';\",\"\")\n",
    "\n",
    "            # Write each speech to a file\n",
    "#             with open(\"C:/Users/Johnny/Desktop/Data Science/Project/speeches/1998/1998_speech_\" + str(count) + \".txt\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "#                 out_file.write(all)\n",
    "            count = count + 1\n",
    "            \n",
    "            # create one sinlge string contain all speeches\n",
    "            speech_string_1998 += all\n",
    "            item = (word_tokenize(all), \"ff\")\n",
    "            speech_list.append(item)\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "#print(speech_string_1998) \n",
    "print(str(count - 1) + \" speeches in 1998\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 74\n",
    "speech_string_1998 = speech_string_1998.lower()\n",
    "words_1998 = word_tokenize(speech_string_1998)\n",
    "\n",
    "print(\"There were %d speeches in 1999 with %d words with an average of %d per speech\" % (count, len(words_1999), len(words_1999)/(count)))\n",
    "print()\n",
    "\n",
    "words_1998 = [w for w in words_1998 if not w in stop_words]\n",
    "all_words_1998 = nltk.FreqDist(words_1998)\n",
    "\n",
    "print(\"There were %d speeches in 1998 with %d words with an average of %d per speech\" % (count, len(words_1998), len(words_1998)/(count)))\n",
    "print()\n",
    "\n",
    "print(all_words_1998.most_common(50))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1997 Speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.taoiseach.gov.ie/eng/News/Archives/1997/Taoiseach's_Speeches_Archive_1997/?pageNumber=\"\n",
    "speech_string_1997 = \"\"\n",
    "count = 1\n",
    "\n",
    "\n",
    "for page in range(1,3):\n",
    "    r = requests.get(base_url + str(page))\n",
    "    c = r.content\n",
    "    soup = BeautifulSoup(c, \"html.parser\")\n",
    "    all = soup.find_all(\"span\",{\"class\":\"ItemName\"})\n",
    "    \n",
    "    for a in all:\n",
    "        try:\n",
    "            r = requests.get(\"https://www.taoiseach.gov.ie\" + a.find(\"a\")[\"href\"])\n",
    "            c = r.content\n",
    "\n",
    "            soup = BeautifulSoup(c, \"html.parser\")\n",
    "            all = soup.find(\"div\", {\"class\":\"contentSub\"}).text\n",
    "            all = all.replace(\"var mapOverlayUrl = '';\",\"\")\n",
    "            \n",
    "            # Write each speech to a file\n",
    "#             with open(\"C:/Users/Johnny/Desktop/Data Science/Project/speeches/1997/1997_speech_\" + str(count) + \".txt\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "#                 out_file.write(all)\n",
    "            count = count + 1\n",
    "            \n",
    "            # create one sinlge string contain all speeches\n",
    "            speech_string_1997 += all\n",
    "            item = (word_tokenize(all), \"ff\")\n",
    "            speech_list.append(item)\n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "#print(speech_string_1997)  \n",
    "print(str(count - 1) + \" speeches in 1997\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(speech_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 11\n",
    "speech_string_1997 = speech_string_1997.lower()\n",
    "words_1997 = word_tokenize(speech_string_1997)\n",
    "\n",
    "print(\"There were %d speeches in 1997 with %d words with an average of %d per speech\" % (count, len(words_1997), len(words_1997)/(count)))\n",
    "print()\n",
    "\n",
    "words_1997 = [w for w in words_1997 if not w in stop_words]\n",
    "all_words_1997 = nltk.FreqDist(words_1997)\n",
    "\n",
    "print(\"There were %d speeches in 1997 with %d words with an average of %d per speech\" % (count, len(words_1997), len(words_1997)/(count)))\n",
    "print()\n",
    "print(all_words_1997.most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_speeches_string = speech_string_1997 + speech_string_1998 + speech_string_1999 + speech_string_2000 \\\n",
    "+ speech_string_2001 + speech_string_2002 + speech_string_2003 + speech_string_2004 + speech_string_2005 \\\n",
    "+ speech_string_2009 + speech_string_2010 + speech_string_2011 + speech_string_2012 + speech_string_2014 \\\n",
    "+ speech_string_2015 + speech_string_2016 + speech_string_2017 + speech_string_2018\n",
    "\n",
    "stop_words.append(\"1997\")\n",
    "stop_words.append(\"1998\")\n",
    "stop_words.append(\"1999\")\n",
    "stop_words.append(\"2000\")\n",
    "stop_words.append(\"2001\")\n",
    "stop_words.append(\"2002\")\n",
    "stop_words.append(\"2003\")\n",
    "stop_words.append(\"2004\")\n",
    "stop_words.append(\"2005\")\n",
    "stop_words.append(\"2006\")\n",
    "stop_words.append(\"2007\")\n",
    "stop_words.append(\"2008\")\n",
    "stop_words.append(\"2009\")\n",
    "stop_words.append(\"2010\")\n",
    "stop_words.append(\"2011\")\n",
    "stop_words.append(\"2012\")\n",
    "stop_words.append(\"2013\")\n",
    "stop_words.append(\"2014\")\n",
    "stop_words.append(\"2015\")\n",
    "stop_words.append(\"2016\")\n",
    "stop_words.append(\"2017\")\n",
    "stop_words.append(\"2018\")\n",
    "all_words = word_tokenize(all_speeches_string)\n",
    "print(len(all_speeches_string))\n",
    "all_words = [w for w in all_words if not w in stop_words]\n",
    "#print(len(all_words))\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "print(all_words.most_common(50))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertie_string = speech_string_1997 + speech_string_1998 + speech_string_1999 + speech_string_2000 \\\n",
    "+ speech_string_2001 + speech_string_2002 + speech_string_2003 + speech_string_2004 + speech_string_2005\n",
    "\n",
    "brian_string = speech_string_2009 + speech_string_2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(leo_string))#802742\n",
    "print(len(enda_string)) #2899919\n",
    "print(len(brian_string)) #731374\n",
    "print(len(bertie_string)) #4989243\n",
    "print((leo_string[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcloud of top 50 words across all speeches\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "top_50_words = [word[0] for word in all_words.most_common(50)]\n",
    "\n",
    "top_50_words_string = \"\"\n",
    "\n",
    "for word in top_50_words:\n",
    "    top_50_words_string += (word + \" \")\n",
    "    \n",
    "print(top_50_words_string)\n",
    "\n",
    "# Create the wordcloud object\n",
    "wordcloud = WordCloud(width=960, height=960, margin=0).generate(top_50_words_string)\n",
    " \n",
    "# Display the generated image:\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.margins(x=0, y=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word(word):\n",
    "    word_count = []\n",
    "    word_count2 = []\n",
    "    print(\"1997 %s appears %d times. %.3f\" % (word, all_words_1997[word], (all_words_1997[word]/len(all_words_1997)*10000)))\n",
    "    print(\"1998 %s appears %d times. %.3f\" % (word, all_words_1998[word], (all_words_1998[word]/len(all_words_1998)*10000)))\n",
    "    print(\"1999 %s appears %d times. %.3f\" % (word, all_words_1999[word], (all_words_1999[word]/len(all_words_1999)*10000)))\n",
    "    print(\"2000 %s appears %d times. %.3f\" % (word, all_words_2000[word], (all_words_2000[word]/len(all_words_2000)*10000)))\n",
    "    print(\"2001 %s appears %d times. %.3f\" % (word, all_words_2001[word], (all_words_2001[word]/len(all_words_2001)*10000)))\n",
    "    print(\"2002 %s appears %d times. %.3f\" % (word, all_words_2002[word], (all_words_2002[word]/len(all_words_2002)*10000)))\n",
    "    print(\"2003 %s appears %d times. %.3f\" % (word, all_words_2003[word], (all_words_2003[word]/len(all_words_2003)*10000)))\n",
    "    print(\"2004 %s appears %d times. %.3f\" % (word, all_words_2004[word], (all_words_2004[word]/len(all_words_2004)*10000)))\n",
    "    print(\"2005 %s appears %d times. %.3f\" % (word, all_words_2005[word], (all_words_2005[word]/len(all_words_2005)*10000)))\n",
    "    print(\"2009 %s appears %d times. %.3f\" % (word, all_words_2009[word], (all_words_2009[word]/len(all_words_2009)*10000)))\n",
    "    print(\"2010 %s appears %d times. %.3f\" % (word, all_words_2010[word], (all_words_2010[word]/len(all_words_2010)*10000)))\n",
    "    print(\"2011 %s appears %d times. %.3f\" % (word, all_words_2011[word], (all_words_2011[word]/len(all_words_2011)*10000)))\n",
    "    print(\"2012 %s appears %d times. %.3f\" % (word, all_words_2012[word], (all_words_2012[word]/len(all_words_2012)*10000)))\n",
    "    print(\"2014 %s appears %d times. %.3f\" % (word, all_words_2014[word], (all_words_2014[word]/len(all_words_2014)*10000)))\n",
    "    print(\"2015 %s appears %d times. %.3f\" % (word, all_words_2015[word], (all_words_2015[word]/len(all_words_2015)*10000)))\n",
    "    print(\"2016 %s appears %d times. %.3f\" % (word, all_words_2016[word], (all_words_2016[word]/len(all_words_2016)*10000)))\n",
    "    print(\"2017 %s appears %d times. %.3f\" % (word, all_words_2017[word], (all_words_2017[word]/len(all_words_2017)*10000)))\n",
    "    print(\"2018 %s appears %d times. %.3f\" % (word, all_words_2018[word], (all_words_2018[word]/len(all_words_2018)*10000)))\n",
    "    word_count.append(all_words_1997[word])\n",
    "    word_count.append(all_words_1998[word])\n",
    "    word_count.append(all_words_1999[word])\n",
    "    word_count.append(all_words_2000[word])\n",
    "    word_count.append(all_words_2001[word])\n",
    "    word_count.append(all_words_2002[word])\n",
    "    word_count.append(all_words_2003[word])\n",
    "    word_count.append(all_words_2004[word])\n",
    "    word_count.append(all_words_2005[word])\n",
    "    word_count.append(all_words_2009[word])\n",
    "    word_count.append(all_words_2010[word])\n",
    "    word_count.append(all_words_2011[word])\n",
    "    word_count.append(all_words_2012[word])\n",
    "    word_count.append(all_words_2014[word])\n",
    "    word_count.append(all_words_2015[word])\n",
    "    word_count.append(all_words_2016[word])\n",
    "    word_count.append(all_words_2017[word])\n",
    "    word_count.append(all_words_2018[word])\n",
    "    word_count2.append((all_words_1997[word]/len(all_words_1997)*100))\n",
    "    word_count2.append((all_words_1998[word]/len(all_words_1998)*100))\n",
    "    word_count2.append((all_words_1999[word]/len(all_words_1998)*100))\n",
    "    word_count2.append((all_words_2000[word]/len(all_words_2000)*100))\n",
    "    word_count2.append((all_words_2001[word]/len(all_words_2001)*100))\n",
    "    word_count2.append((all_words_2002[word]/len(all_words_2002)*100))\n",
    "    word_count2.append((all_words_2003[word]/len(all_words_2003)*100))\n",
    "    word_count2.append((all_words_2004[word]/len(all_words_2004)*100))\n",
    "    word_count2.append((all_words_2005[word]/len(all_words_2005)*100))\n",
    "    word_count2.append((all_words_2009[word]/len(all_words_2009)*100))\n",
    "    word_count2.append((all_words_2010[word]/len(all_words_2010)*100))\n",
    "    word_count2.append((all_words_2011[word]/len(all_words_2011)*100))\n",
    "    word_count2.append((all_words_2012[word]/len(all_words_2012)*100))\n",
    "    word_count2.append((all_words_2014[word]/len(all_words_2015)*100))\n",
    "    word_count2.append((all_words_2015[word]/len(all_words_2016)*100))\n",
    "    word_count2.append((all_words_2016[word]/len(all_words_2017)*100))\n",
    "    word_count2.append((all_words_2017[word]/len(all_words_2018)*100))\n",
    "    print(word_count)\n",
    "    print(word_count2)\n",
    "#find_word(\"bailout\")\n",
    "#find_word(\"anglo\")\n",
    "#find_word(\"recapitalisation\")\n",
    "# find_word(\"luas\")\n",
    "# find_word(\"tunnel\")\n",
    "# find_word(\"broadband\")\n",
    "# find_word(\"ceasefire\")\n",
    "#\n",
    "#find_word(\"terrorist\")\n",
    "#find_word(\"housing\")\n",
    "#find_word(\"marriage\")\n",
    "#find_word(\"boom\")\n",
    "#find_word(\"abuse\")\n",
    "#find_word(\"election\")\n",
    "find_word(\"backstop\")\n",
    "find_word(\"luas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# historical events\n",
    "## luas opening\n",
    "## ceasefire late nineties 2002 threathened\n",
    "## brexit\n",
    "## abortion\n",
    "## terrorist\n",
    "## tribunals\n",
    "## housing\n",
    "## unemployemt\n",
    "## marriage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_notebook()\n",
    "year = [1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2009, 2010, 2011, 2012, 2014, 2015, 2016, 2017, 2018]\n",
    "unemployment = [0, 24, 42, 44, 15, 17, 19, 10, 35, 3, 4, 8, 25, 44, 68, 29, 12, 11]\n",
    "q = figure()\n",
    "q.line(year, unemployment,  legend=\"unemployment\", color=\"red\")\n",
    "\n",
    "#q.line(year, growth,  legend=\"growth\", color = \"green\")\n",
    "\n",
    "show(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gov doesnt like to talk about unemployment when it's high but talks a lot about it when it's low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_notebook()\n",
    "year = [1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2009, 2010, 2011, 2012, 2014, 2015, 2016, 2017, 2018]\n",
    "emigration = [0.0, 0.08596604341285193, 0.1182033096926714, 0.09260331056835282, 0.14840182648401826, 0.1245674740484429, 0.11245431543435479, 0.03273322422258593, 0.04943968358602505, 0.027225701061802342, 0.0, 0.044836347332237333, 0.06024096385542169, 0.0, 0.009379983116030392, 0.06173474637308365, 0.01938172303517783, 0.05064573309698658]\n",
    "immigration = [0.0, 0.02149151085321298, 0.06447453255963895, 0.06945248292626462, 0.0684931506849315, 0.06920415224913494, 0.028113578858588697, 0.08183306055646482, 0.05767963085036256, 0.04083855159270351, 0.0, 0.029890898221491553, 0.030120481927710847, 0.0, 0.02813994934809117, 0.08231299516411153, 0.08721775365830023, 0.07596859964547988]\n",
    "growth = [0.6564551422319475, 0.9563722329679777, 0.8596604341285192, 1.0649380715360575, 0.6050228310502284, 0.5397923875432525, 0.7590666291818948, 1.309328968903437, 1.441990771259064, 0.7759324802613667, 1.2520638414969731, 1.150799581527425, 2.6656626506024095, 0.09278589654372535, 0.7597786323984617, 1.6153925300956888, 0.7655780598895242, 0.7470245631805521]\n",
    "q = figure()\n",
    "q.line(year, emigration,  legend=\"emigration\", color=\"red\")\n",
    "q.line(year, immigration,  legend=\"immigration\", color = \"blue\")\n",
    "#q.line(year, growth,  legend=\"growth\", color = \"green\")\n",
    "\n",
    "show(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.taoiseach.gov.ie/eng/News/Taoiseach's_Speeches/?pageNumber=\"\n",
    "count = 1\n",
    "for page in range(1,38):\n",
    "    r = requests.get(base_url + str(page))\n",
    "    c = r.content\n",
    "    soup = BeautifulSoup(c, \"html.parser\")\n",
    "    dates = soup.find_all(\"span\",{\"class\":\"newsDate\"})\n",
    "    years = soup.find_all(\"span\",{\"class\":\"newsYear\"})\n",
    "    for date, year in zip(dates, years):\n",
    "        date = re.split('(\\d+)', date.text)\n",
    "        date = date[1] + \" \" + date[2] + \" \" + year.text\n",
    "        print(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, output_notebook, show\n",
    "\n",
    "output_notebook()\n",
    "year = [1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013]\n",
    "no_of_speeches = [11, 74, 77, 73, 65, 73, 34, 30, 152, 0, 0, 2, 41, 46, 38, 44, 368]\n",
    "q = figure()\n",
    "q.circle(year, no_of_speeches, size = 20, alpha = 0.4)\n",
    "\n",
    "show(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "year = [\"97\", \"98\", \"99\", \"00\", \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\"]\n",
    "brexit = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  4, 114, 225, 73]\n",
    "\n",
    "p = figure(x_axis_label = \"Year\", x_range = year, title = \"Occurences of Brexit\", plot_height = 200)\n",
    "p.vbar(x = year, top = brexit, width = 0.9,)\n",
    "       \n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Occurences of Brexit as a % of all words of that year\n",
    "year = [\"97\", \"98\", \"99\", \"00\", \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\"]\n",
    "brexit = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  0.006, 0.195, 0.401, 0.167]\n",
    "\n",
    "p = figure(x_axis_label = \"Year\", x_range = year, title = \"Occurences of Brexit\", plot_height = 200)\n",
    "p.vbar(x = year, top = brexit, width = 0.9,)\n",
    "       \n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Occurences of the word recession\n",
    "year = [\"97\", \"98\", \"99\", \"00\", \"01\", \"02\", \"03\", \"04\", \"05\", \"09\", \"10\", \"11\", \"12\", \"14\", \"15\", \"16\", \"17\", \"18\"]\n",
    "recession = [0, 0, 1, 2, 0, 0, 0, 1, 0, 21, 21, 2, 1, 10, 33, 1, 3, 3]\n",
    "\n",
    "p = figure(x_axis_label = \"Year\", x_range = year, title = \"Occurences of recession\", plot_height = 200)\n",
    "p.vbar(x = year, top = recession, width = 0.9,)\n",
    "       \n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Occurences of the word recession % 0f total words\n",
    "year = [\"97\", \"98\", \"99\", \"00\", \"01\", \"02\", \"03\", \"04\", \"05\", \"09\", \"10\", \"11\", \"12\", \"14\", \"15\", \"16\", \"17\", \"18\"]\n",
    "recession = [0, 0, recession_1999, recession_2000, 0, 0, 0, recession_2004, 0, recession_2009, recession_2010, recession_2011, recession_2012, recession_2014, recession_2015, recession_2016, recession_2017, recession_2018]\n",
    "\n",
    "p = figure(x_axis_label = \"Year\", x_range = year, title = \"Occurences of recession\", plot_height = 200)\n",
    "p.vbar(x = year, top = recession, width = 0.9,)\n",
    "       \n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recession\n",
    "output_notebook()\n",
    "year = [1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2009, 2010, 2011, 2012, 2014, 2015, 2016, 2017, 2018]\n",
    "recession = [0, 0, 1, 2, 0, 0, 0, 1, 0, 21, 21, 2, 1, 10, 33, 1, 3, 3]\n",
    "recovery = [0, 4, 2, 3, 7, 4, 4, 6, 6, 46, 54, 35, 79, 140, 280, 105, 27, 1]\n",
    "growth = [9, 89, 80, 92, 53, 39, 54, 80, 175, 57, 91, 77, 177, 81, 157, 79, 59, 56]\n",
    "investment = [2, 96, 107, 139, 70, 100, 66, 78, 143, 86, 107, 40, 109, 132, 181, 130, 142, 104]\n",
    "brexit = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 114, 225, 63]\n",
    "q = figure()\n",
    "q.line(year, recession,  legend=\"recession\", color=\"red\")\n",
    "q.line(year, recovery,  legend=\"recovery\", color = \"blue\")\n",
    "q.line(year, growth,  legend=\"growth\", color = \"green\")\n",
    "q.line(year, investment,  legend=\"investment\", color = \"orange\")\n",
    "q.line(year, brexit,  legend=\"brexit\", color = \"brown\")\n",
    "\n",
    "show(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recovery\n",
    "output_notebook()\n",
    "year = [1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2009, 2010, 2011, 2012, 2014, 2015, 2016, 2017, 2018]\n",
    "recovery = [0, 4, 2, 3, 7, 4, 4, 6, 6, 46, 54, 35, 79, 140, 280, 105, 27, 1]\n",
    "recovery2 = [0, 4.298, 2.098, 3.473, 7.991, 5.536, 5.623, 9.820, 4.944, 62.619, 74.298, 52.309, 118.976, 162.375, 262.64, 108.036, 26.165, 1.266]\n",
    "q = figure()\n",
    "q.line(year, recovery,  legend=\"r\", color=\"red\")\n",
    "q.line(year, recovery2,  legend=\"r2\", color = \"blue\")\n",
    "show(q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_words = \"ABLE, ACCEPT , ACCEPTANCE , ACCEPTABLE , ACCEPTED , ACCEPTING, ACTION, ACTIVATE, ACTIVE, ADD, ADDITION, ADORABLE, ADVANTAGE, AFFIRM, AGELESS, AGREE, AGREEABLE, AID, AIM, ABUNDANCE, ACCOUNTABILITY, ACCOMPLISHMENT , ACCOMPLISH, ACCURACY, ACHIEVEMENT , ACHIEVE, ACKNOWLEDGEMENT, ADAPTABILITY, ADVENTURE , ADVENTUROUS, AGILITY, ALERTNESS, AMBITION, ANTICIPATION, APPRECIATE , APPRECIATION , APPRECIATIVE , APPRECIATIVENESS, ASSERTIVENESS , ASSERTIVE, ATTENTIVENESS, AUDACITY, AWARE , AWARENESS, AUTHENTIC , AUTHENTICITY, ABRACADABRA, ATTRACTION, ALLOW , ALLOWING, AFFECTION , AFFECTIONATE, ABSORBED, ALERT, AMAZED, AWE , AWED, ANIMATE , ANIMATED , ANIMATING , ANIMATION, ANIMATENESS, ARDENT, AMAZING, AWESOME â€“ AWESOMENESS, AROUSED, ASTONISHED â€“ ASTONISHING, AMUSED, AIR â€“ AIRNESS, ALOHA, ADORE, ADMIRE, ADMIRABLE, ALLURE, ANGEL â€“ ANGELIC, ALTRUISM â€“ ALTRUISTIC, ABOUND â€“ ABOUNDING â€“ ABOUNDS- ABUNDANT, ABSOLUTE â€“ ABSOLUTELY, ACCESSIBLE, ACCLAIMED, ACCOMMODATE â€“ ACCOMMODATED â€“ ACCOMMODATION â€“ ACCOMMODATING, AMPLE, APPRECIATIVE JOY, AMIN, ACCENTUACTIVITY, ACTABILITY, AFFABLE, ALACRITY, ALTRUCAUSE, AMIABLE, ASTOUNDING, ATTRACTIVE, ALIVE , ALIVENESS, ACCLAIM, ACCLAMATION, ACCOMPLISHED, ACCOMPLISHMENTS, ACCURATE, ACCURATELY, ACHIEVABLE, ACHIEVEMENTS, ACTION FOR HAPPINESS, ACTIVE AND CONSTRUCTIVE STEPS, ACTS OF KINDNESS, ADAPTABLE, ADAPTIVE, ADEQUATE, ADMIRABLY, ADMIRATION, ADMIRED, ADORED, ADORING, ADORINGLY, ADVANCED, ADVANTAGEOUS, ADVANTAGEOUSLY, ADVANTAGES, AFFABILITY, AFFABLY, AFFINITY, AFFIRMATION, AFFIRMATIVE, AFFLUENCE, AFFLUENT, AFFORD, AFFORDABLE, AFFORDABLY, AGILE, AGILELY, AGREEABLENESS, AGREEABLY, ALIGNED, ALLURING, ALLURINGLY, ALTRUISTICALLY, AMAZE, AMAZEMENT, AMAZES, AMAZINGLY, AMIABILITY, AMICABILITY, AMICABLE, AMICABLY, AMUSING, APPEAL, APPEALING, APPLAUD, APPRECIABLE, APPRECIATED, APPRECIATES, APPRECIATION OF BEAUTY, APPRECIATIVELY, APPROPRIATE, APPROVAL, APPROVE, ARDOR, ART OF APPRECIATION, ART OF STILLNESS, ART OF WELL-BEING, ASSURANCE, A REASON FOR BEING, ACARONAR, ACCOMMODATIVE, ALTITUDINARIAN,  AMIABLY, ACCOLADE, ACUMEN, ADJUSTABLE, ADMIRER, ADMIRING, ADMIRINGLY, ADORER, ADROIT, ADROITLY, ADULATED, ADULATION, ADULATORY, ADVENTURESOME, ADVOCATED, AMBITIOUS, AMBITIOUSLY, AMELIORATE, AMENITY, AMITY, AMPLY, AMUSE, AMUSINGLY, APOTHEOSIS, ASSUME YOUR OWN VALUE, ASTONISHINGLY, ASTONISHMENT,  AWAKEN, AWAKENING,  AURORA, BEATIFY, BEATITUDE, BENEFICIAL, BENEFIT, BENEVOLENT, BELOVED, BEST, BETTER, BLESS , BLESSING , BLESSED, BLISS , BLISSFULNESS , BLISSFUL, BLOOM, BLOSSOM, BALANCE , BALANCED, BEAUTY , BEAUTIFUL , BEAUTIFULLY, BELONG , BELONGING, BOLDNESS, BRAVERY, BRILLIANCE , BRILLIANT,  BIOPHILIA, BRIGHT , BRIGHTEN, BRIGHTNESS, BALISTIC, BLASTING, BLAZING, BLINDING, BREATHTAKING, BUBBLING, BUSTING, BLISSCIPLINE, BUYANCY, BULLISHNESS, BRISKNESS, BUOYANCY, BREEZINESS, BRIO,  BEAUTIFY,  BENEFACTOR, BENEFITS, BENEVOLENCE, BENEVOLENTLY,   BEYOND, BEAUTY IN ALL THINGS, BEINGNESS, BELIEVABLE, BLOOD-BROTHERS,  BADASSERY, BEST-SELLING,  BETTER-KNOWN,  BLITHESOME, BLOSSOMING, BONUS, BLING, BUDO, BLASTING LOVE, BUDDHAHOOD, CARE â€“ CARING, CALM, CREATE, CREATIVE , CREATIVITY â€“ CREATIVENESS , CAPABLE â€“ CAPABILITY â€“ CAPABLY, CELEBRATE â€“ CELEBRATION, CERTAIN â€“ CERTAINTY, CHARITABLE â€“ CHARITY, CHARM â€“ CHARMING â€“ CHARMER, CHOICE, CLEAN , CLEANLINESS, COMFORT , COMFORTABLE ,COMFORTING, CUDDLE , CUDDLING, CANDOR, CAREFULNESS, CHALLENGE, CHANGE, CHEERFUL , CHEERFULNESS, CLARITY, COLLABORATION, COMMITMENT, COMMUNICATION, COMMUNITY, COMPASSION , COMPASSIONATE, COMPETENT , COMPETENCE â€“ COMPETENCY, CONCENTRATION, CONFIDENT , CONFIDENCE, CONSCIOUSNESS, CONSISTENCY , CONSISTENT, CONTENT , CONTENTMENT, CONTINUITY , CONTINUOUS, CONTRIBUTION, CONVICTION , CONVINCING, COOPERATION, COURAGE, COURTESY , COURTEOUS, CURIOUS , CURIOSITY, CHAKRA, COOL, CLEAR HEADED, CENTERED, CLOSENESS, COMPANIONSHIP, CONSIDERATE â€“ CONSIDERATION, COMMUNION, CONNECT â€“ CONNECTED â€“ CONNECTION â€“ CONNECTEDNESS, CONQUER, CUTE, CHARISMA â€“ CHARISMATIC, COLLECTED, CHEERFUL WILLINGNESS, CHEERS, CONGRUENCE, CORDIAL, CAPITAL, CORKING, CLEAR, CARESS, CHEERFUL MOOD,  CONTENTED, COZINESS, CUTENESS, CAREFREENESS, CAREFREE, CENTERING, CENTERING MEDITATION, CITIZEN OF MASTERY, CO-CREATING, CO-CREATOR, COHESION, CONTINUAL STREAM OF SYNCHRONICITY, CREATIVE PROCESS, CREATIVE AFFIRMATIONS, COMPOSTURE, CONCORD, CEREBRO,  CHI, CLASSY, COPACABANA, COSMIC AWARENESS, DIRECTION, DELICATE, DECENT, DESIRABLE, DELICIOUS , DELICIOUSNESS, DO, DREAM â€“ DREAMY, DYNAMIC, DARING, DECISIVENESS, DELIGHT â€“ DELIGHTED â€“ DELIGHTFUL, DEPENDABILITY, DESIRE, DETERMINATION, DEVOTION, DIGNITY, DILIGENCE, DISCIPLINE, DISCOVERY, DISCRETION, DIVERSITY, DRIVE, DUTY, DIVINE, DAZZLED, DISNEY, DEVOTED, DANDY, DAIMON, DEBONAIR, DETACHMENT, DEDICATED, DAUWTRAPPEN, DAZZLE, DELIGHTFULLY, DEFENCELESSNESS,  DESERVE, DESERVEDNESS, DESERVINGNESS, DIS-IDENTIFY, DOPE, EMPATHY , EMPATHIZE , EMPHATIC, EASY , EASILY , EASIER, EDUCATE , EDUCATION , EDUCATED, EFFICIENT, ENABLE , ENABLED, ENERGETIC â€“ ENERGIZE â€“ ENERGY, ENGAGE â€“ ENGAGING , ENGAGED, ENJOY , ENJOYMENT, ENOUGH, EAGER , EAGERNESS, EFFECTIVENESS, EFFICIENCY, ELATION, ELEGANCE, ENCOURAGE , ENCOURAGEMENT â€“ ENCOURAGED, ENDURANCE, EQUALITY, EXCELLENCE,â€“ EXCELLENT, EXCITE, EXCITEMENT , EXCITED, EXPERIENCE, EXPERTISE, EXPLORATION, EXPRESSIVENESS , EXPRESSING, ENLIGHTENMENT , ENLIGHTENED, ETERNAL, EXALTATION, EMULATE, EMPOWER , EMPOWERING , EMPOWERED, EXPANSIVE, EXHILARATING, ENTHUSIASTIC , ENTHUSIASM, ENGROSSED, ENCHANTED, ENTRANCED, ECSTATIC, ELATED, ENTHRALLED, EXUBERANT , EXUBERANCE, EXPECTANT, EQUANIMOUS, ENLIVENED, EFFICACY, EASE, EXEMPLARY, EXTRAORDINARY, EARNEST, ELEVATE â€“ ELEVATED, EQUANIMITY, EASE-OF-MIND, EXCITED ANTICIPATION, EXTRA, EQUITY , EQUITABLY , EQUITABLE, ECSTATIFY, EUDAEMONISM, EUDAEMONIST, EUDAEMONISTIC, EUDAIMONIA, EUDAMONIA, EVOLVE, EXALTING, EXSTATISFY, EXULTANT, ASTRONOMICAL, CHAMPION, CHAMPâ€™, ELECTRIC, ENORMOUS , EXCEPTIONAL, EXCITING, EXQUISITE, EFFORTLESSNESS, EUNOIA, ECOSOPHY, EBULLIENCE, EMBRACE,   ERLEBNIS,  EFFORTLESSLY, EKAGGATA,  EARTHING, EVER-JOYOUS,  ETHEREAL, ENDLESS, FANTASTIC,  FLOW , FLOWING, FABULOUS, FAIR, FAITH, FAITHFUL, FAME, FAVORITE, FAIRNESS, FAMILY, FIDELITY, FLEXIBILITY, FOCUS, FLOURISH, FORGIVE â€“ FORGIVING â€“ FORGIVENESS, FORTITUDE, FREE , FREEDOM, FRUGALITY, FUN, FUTURE, FRIEND , FRIENDLY , FRIENDSHIP , FRIENDLINESS, FASCINATE , FASCINATED, FULFILL , FULFILLED, FOOD, FEISTY , FEISTINESS, FEASIBLE, FINE, FEARLESS, FESTIVE , FESTIVENESS, FIT, FANTABULOUS, FREECYCLE, FUNERIFIC, FUNOLOGY, FLAWLESS, FAMOUS, FANCY, FLASHY, FTW, FUNNY JOKES, FLAUNTING, FONDLE, FRIC-TIONLESSLY, FLAWLESSLY, FLOURISHING, FORTUITOUS, FUN-LOVING, FREE-SPIRITED, FELICITY, GLOW, GENEROUS â€“ GENEROSITY, GENERATE, GENIAL, GENIUS, GENUINE, GIFT, GIVE ,GIVING, GOOD, GOODNESS,  GRACE, GRATITUDE, GRATEFULNESS, GROW , GROWTH, GUIDE , GUIDING , GUIDANCE, GOD, GROUNDED, GLORY, GODLINESS,  GROOVY, GIDDY, GLAD,  GLAMOR, GIGGLING, GODDESS, GORGEOUS , GORGEOUSNESS, GRANDIOSITY, GENERAVITY, GENTLEMAN, GARGANTUAN, GRAND, GREAT, GINGER, GOOD-HUMORED, GOODWILL, GREATFUL, GEMUTLICHKEIT, GIBIGIANA, GIGIL, GOOD INDWELLING SPIRIT, GOOD-HUMORED, GOODWILL,  GAME-CHANGER, GENERATOR OF LIFE, GRACEFULLY, GRACIOUSNESS, GOLDILOCKS, GENUINENESS, GREAT ZEAL, GOOD DONE IN SECRET, HOPE , HOPEFULNESS, HAPPINESS , HAPPY , HAPPILY, HARMONIOUS , HARMONIZE , HARMONY, HEALTH , HEALTHY, HEART, HELLO, HELP , HELPFUL , HELPING, HOT , HONEST , HONESTY, HUMAN, HUMOR, HELPFULNESS, HERO , HEROISM, HOLY , HOLINESS, HONOR, HOSPITALITY, HUMBLE, HEAVEN , HEAVENLY, HALO, HEARTFELT, HEARTWARMING, ONE-POINTEDNESS, HAPPY HEARTED, HEEDFUL, HANDSOME, HUGE, HIGH-SPIRITEDNESS,  HEART-OPENING, HOSPITABLE, HUMAN FLOURISHING, HIGHLY DISTINGUISHED, HARNESS, HEIGHTENED, HOLISTIC, HOLY SPIRIT, HALL OF AWESOMENESS, HONEY BADGER,  HALYCON, HABITUATION,  IMAGINATION, INSPIRE , INSPIRATION , INSPIRED , INSPIRATIONAL,  IDEA, INCREDIBLE, INNOVATE , INNOVATION, INTERESTING â€“ INTEREST â€“ INTERESTED, IMPROVEMENT, INDEPENDENCE, INFLUENCE, INGENUITY, INNER PEACE, INSIGHT â€“ INSIGHTFULNESS â€“ INSIGHTFUL, INTEGRITY, INTELLIGENCE â€“ INTELLIGENT, INTENSITY, INTIMACY, INTUITIVENESS, INVENTIVENESS, INVESTING, INTENTION, INVIGORATE â€“ INVIGORATED, INTRIGUED, INVOLVE â€“ INVOLVED, INCLUSION, INNOCENT, INEFFABLE â€“ INEFFABILITY, INTREPID, IDEALISM, ILLUMINATION â€“ ILLUMINATED, INCOMPARABLE, INVINCIBLE, INQUISITIVE, INFINITE, INFINITY, ILLUSTRIOUS, INNER, ICHARIBA CHODE, IKIGAI, INCREDIBLE CUTENESS, INDWELLING,  IRIDESCENT, ILLUSTRIOUS, INNER,  INTERCONNECTED, INTERCONNECTIVITY, INTUITION, INCLUSIVENESS,JOY , JOYFUL , JOYOUS, JOKE, JOLLY, JOVIAL, JUST, JUSTICE, JUBILANT, JUVENESCENT, JUMPY, JAMMIN, JUBILINGO, KINDNESS , KIND ,KIND-HEART , KINDLY, KEEP-UP, KISS, KNOWLEDGE, KITTENS, KEEN, KAAJHUAB, KALON, KILIG,  LIKE, LAUGH , LAUGHING, LEARN , LEARNING, LIFE, LIVE â€“ LIVING, LUXURY, LONGEVITY, LOYALTY â€“ LOYAL, LOVE â€“ LOVABLE â€“ LOVING, LIBERTY, LOGIC, LEADER , LEADERSHIP, LUCK , LUCKY, LIGHT,  LIVELY,  LOVELY,  LIGHTWORKER, LEADING,  LUSTROUS,  LIGHT-HEARTED, LEEWAY,  LIVELINESS, MEANING , MORE, MAGNIFICENT, MAJESTY, MANY, MARVELOUS, MERIT, MOTIVATE, MIRACLE, MAGIC, MASTERY, MATURITY, MINDFUL â€“ MINDFULNESS, MODESTY, MOTIVATION â€“ MOTIVATIONAL, MERCY, MEDITATION, MIND-BLOWING, MELLOW, MOVED, MOVEMENT, MUTUALITY, MOURNING, MELIORISM, MENCH, MINDSIGHT, MINDSIGHT, MAJOR, MILD, MEANINGFUL, MEMORABLE, MORPHING, MOTIVATED, MOVING,  MIRTHFUL, MYRIAD, MOJO, NOBLE, NURTURING , NURTURE, NON-RESISTANCE , NON-RESISTANT, NEW, NICE, NIRVANA, NEAT,  NOURISH , NOURISHED , NOURISHING , NOURISHMENT, NAMASTE, NEOTENY,  NOVATURIENT, NON-DUALITY,OPTIMIST , OPTIMISTIC, OUTSTANDING, OK, ON, ONWARDS, OPEN , OPENLY , OPENING, OPEN-MINDED, OPPORTUNITY, ORIGINAL, OPENNESS, OPTIMISM, ORDER, ORGANIZATION, ORIGINALITY, OUTCOME, ORIENTATION, OBEDIENT,  OMG, OVERCOME,  OUTGOING, ONENESS, OUTERNATIONALIST,  ORENDA,  OMNISCIENCE,  PERFECT , PERFECTION,  PEACE , PACIFY, PARADISE , PARADISIAC, PASSION , PASSIONATE, PLEASE, PURE, PERCEPTIVENESS, PERSEVERANCE, PERSISTENCE,  PLEASURE,  POWER , POWERFUL, PRACTICALITY, PRECISION, PREPAREDNESS, PRESENCE, PRESERVATION, PRIVACY, PROACTIVITY , PROACTIVE, PROGRESS, PROSPERITY,  PROSPEROUS, PUNCTUALITY , PUNCTUAL, PATIENCE, PROUD, PLEASED, PLAY â€“ PLAYFUL â€“ PLAYFULNESS, PARTICIPATION, PURPOSE, PICK-ME-UP, PRONIA, PIOUS, PUPPIES, POLITE, POSITIVE MIND, POSITIVE THINKING, PRETTY, PRECIOUS, PARDON, PERKINESS, PIQUANCY, POSICHOICE, POSIDRIVING, POSIFIT, POSILENZ, POSIMASS, POSIMINDER, POSIRATIO, POSIRIPPLE, POSIRIPPLER, POSIRIPPLES, POSISINGER, POSISITE, POSISTRENGTH, POSITIBILITARIAN, POSITRACTION, POSITUDE, POSIVALUES, POSIWORD, POSSIBILITARIAN, PROMPTNESS, PROTO, PRICELESS, PEP â€“ PEPPINESS, PERMALICIOUS, PLUCKY, POLLYANNAISM, PRIDE, PETRICHOR, PHILOCALIST,  PROTECT, POLITENESS,  PRIVILEGE, PROPITIOUS,  PICTURESQUE, PRANA, PANACHE, QUALITY, QUIET , QUIETNESS, QUAINT, QUIESCENT, QUEENLY, QUICKENING, QUIDDITY,   QUANTUMNESS,  RESPECT, RADIANT, READY , READINESS, REAL , REALITY, REASON, RECOMMEND, REFRESH â€“ REFRESHED, RELAX â€“ RELAXED, RELIEF, RELIEVE â€“ RELIEVED, REMARKABLE, RATIONALITY, RECOGNITION, RELATIONSHIPS, RELIABLE â€“ RELIABILITY, RELIGION, RESOURCEFULNESS, RESPONSIBILITY, RIGHTEOUSNESS, RISK-TAKING, ROMANCE, REVELATION, REVIVED, RESTORE â€“ RESTORED, REST â€“ RESTED, RENEW â€“ RENEWED, REJUVENATE â€“ REJUVENATED, RAPTURE â€“ RAPTUROUS, RESILIENT â€“ RESILIENCE, REVERENCE, RIPE, REBORN, RELATEDNESS, RASASVADA, REPOSE, ROSINESS, RELENT, RENOWNED, RESPECTED, RAINBOW, ROMANTIC, RELENT, RENOWNED, RADIATE, RECOGNIZE, RELEASING, RIGHTFUL, ROCKSTAR, SCOPE, SMILE â€“ SMILING, SOULMATE, SOUL â€“ SOULFUL, SACRED, SAFE â€“ SAFETY, SECURE â€“ SECURED â€“ SECURITY, SUSTAIN â€“ SUSTAINED, SAVE, SAVINGS, SIMPLE â€“ SIMPLIFY, SELFLESSNESS, SELF-ESTEEM, SERVICE, SIMPLICITY, SINCERITY, SKILL â€“ SKILLED, SPIRIT, SERENE â€“ SERENITY, STABILITY, STRENGTH, STYLE, SYSTEMATIZATION, SELF-LOVE, STRIVE, SALVATION, SELF-RESPECT, SELF-FORGIVENESS, SERVE, SYMPATHETIC, SELF-COMPASSION, SELF-KINDNESS, SPELLBOUND, STIMULATED â€“ STIMULATING â€“ STIMULATION, SATISFIED, STILL, SURPRISED, SLEEP, SEXUAL EXPRESSION, SHELTER, SELF-EXPRESSION, SPACE â€“ SPACIOUS, SPONTANEITY â€“ SPONTANEOUS, SUNSHINE, SPARK, SPARKLE , SPARKLES, SWEET , SWEETNESS, SUPPORT, SUPPORTING , SUPPORTED, SEXY , SEXINESS, SUPREME, SUCCULENT, SWEETHEART, STUDY â€“ STUDIOUS, SAVOUR â€“ SAVOURING, SUFFICIENT, STUPENDOUS, SWAG , SWAGGY, SPLENDID, SMART, SPECTACULAR, SPECIAL, SERENDIPITY, SYNERGY,  START, STEADFASTNESS, SUBLIME, SUNNINESS, SUPERPOWER, SPUNKY, STELLAR, SUPERCHARGE, SUPERCHARGED,  SYNCHRONICITY, SASSY, SUPERCALIFRAGILISTIC, SUPERCALIFRAGILISTICEXPIALIDOCIOUS, TRUE, TRUST , TRUSTING , TACT, TEACH â€“ TEACHABLE, TEAM, THANKFUL, THANK ,THANKFULNESS, THERAPY, TIME, TEAMWORK, TIMELINESS, TOLERANCE, TRADITION, TRANQUIL , TRANQUILITY, TRUTH , TRUTHFULNESS, TENDER, THRILLED, TOUCH , TOUCHED, TICKLED,  TRANSFORMATIVE , TRANSFORMATION , TRANSFORM, TRIUMPH, THRIVE , THRIVING, TENACITY, TRANSPARENT, TEMUL, TENDERLY, TIDSOPTIMIST, UNIFICATION, UNIQUE, UPLIFT, ULTIMATE, UNCONDITIONAL, UPGRADE, USEFUL, USER-FRIENDLY, UNITY, UNDERSTAND , UNDERSTANDING ,UNDERSTOOD, UPSKILL, UNBELIEVABLE, UNFLAPPABLE, UNREAL, UNABASHED,   UNHURRY, UNBELIEVABLE, UNFLAPPABLE, UNREAL, VITALITY, VALUE â€“ VALUES â€“ VALUABLE, VIRTUOUS, VALID, VERIFY, VERY, VIABLE, VIRTUE, VICTORY , VICTORIOUS, VARIETY, VULNERABILITY ,VULNERABLE, VIBRANT, VOW, VIM, VIGOR, VENERATION, VOCABULEVERAGE, VERSATILITY, UBUNTU, WORTH â€“ WORTHY â€“ WORTHINESS, WEALTH, WARM â€“ WARMTH, WELCOME,  WILLING , WILLINGNESS , WISDOM, WISE, WON, WONDERFUL,  WHOLEHEARTEDNESS, WOW, WONDER, WATER, WELL, WELLNESS, WELFARE, WHOLE,  WIN , WINNABLE , WINNING, WALWALUN, WHOLEHEARTEDLY,  WONDROUS, WANDERLUST, XO, XENODOCHIAL, XFACTOR, XENOPHILE, XENIAL, YES, YOUTH, YOUTHFUL, YOUNG, YIPPEE, YAY, YEARN, YEA, YEAH, YUMMY, YEN, YESABILITY, YUGEN, YARAANA, YESABLE, ZEALOUS, ZEAL, ZEST, ZESTY, ZESTFUL, ZIPPY, ZING, ZAPPY, ZANY, ZAJEBISCIE\"\n",
    "# pos_words = pos_words.lower()\n",
    "# pos_words = word_tokenize(pos_words)\n",
    "\n",
    "# # remove all instances of \",\"\n",
    "# pos_words = [word for word in pos_words if word != \",\"]\n",
    "# pos_words = [word for word in pos_words if word != \"â€“\"]\n",
    "# pos_words = [w for w in pos_words if not w in stop_words]\n",
    "\n",
    "# #print(\"There are %s positive words\" % len(pos_words))\n",
    "# all_pos_words = [w for w in pos_words if w in speech_string_2010]\n",
    "# print(len(all_pos_words))\n",
    "# #print(all_pos_words)\n",
    "# freq_pos_words = nltk.FreqDist(all_pos_words)\n",
    "# print(len(freq_pos_words))\n",
    "# print(freq_pos_words.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pos words 2nd dataset\n",
    "pos_words=\"absolutely,accepted,acclaimed,accomplish,accomplishment,achievement,action,active,admire,adorable,adventure,affirmative,affluent,agree,agreeable,amazing,angelic,appealing,approve aptitude attractive awesome,beaming,beautiful,believe,beneficial,bliss,bountiful,bounty,brave,bravo,brilliant,bubbly,calm,celebrated,certain,champ,champion,charming,cheery,choice,classic,classical,clean,commend,composed,congratulation,constant,cool,courageous,creative,cute,dazzling,delight,delightful,distinguished,divine,earnest,easy,ecstatic,effective,effervescent,efficient,effortless,electrifying,elegant,enchanting,encouraging,endorsed,energetic,energized,engaging,enthusiastic,essential,esteemed,ethical,excellent,exciting,exquisite,fabulous,fair,familiar,famous,fantastic,favorable,fetching,finefitting,flourishing,fortunate,free,fresh,friendly,fun,funny,generous,genius,genuine,giving,glamorous,glowing,good,gorgeous,graceful,great,green,grin,growing,handsome,happy,harmonious,healing,healthy,hearty,heavenly,honest,honorable,honored,hug,idea,ideal,imaginative,imagine,impressive,independent,innovate,innovative,instant,instantaneous,instinctive,intellectual,intelligent,intuitive,inventive,jovial,joy,jubilant,keen,kind,knowing,knowledgeable,laugh,learned,legendary,light,lively,lovely,lucid,lucky,luminous,marvelous,masterful,meaningful,merit,meritorious,miraculous,motivating,moving,natural,nice,novel,now,nurturing,nutritious,okay,open,optimistic,paradise,perfect,phenomenal,pleasant,pleasurable,plentiful,poised,polished,popular,positive,powerful,prepared,pretty,principled,productive,progress,prominent,protected,proud,quality,quick,quiet,ready,reassuring,refined,refreshing,rejoice,reliable,remarkable,resounding,respected,restored,reward,rewarding,right,robust,safe,satisfactory,secure,seemly,simple,skilled,skillful,smile,soulful,sparkling,special,spirited,spiritual,stirring,stunning,stupendous,success,successful,sunny,super,superb,supporting,surprising,terrific,thorough,thrilling,thriving,tops,tranquil,transformative,transforming,trusting,truthful,unreal,unwavering,upupbeat,upright,upstanding,valued,vibrant,victorious,victory,vigorous,virtuous,vital,vivacious,wealthy,welcome,well,whole,wholesome,willing,wonderful,wondrous,worthy,wow,yes,yummy,zeal,zealous\"\n",
    "pos_words = word_tokenize(pos_words)\n",
    "\n",
    "# remove all instances of \",\"\n",
    "pos_words = [word for word in pos_words if word != \",\"]\n",
    "pos_words = [w for w in pos_words if not w in stop_words]\n",
    "\n",
    "#print(\"There are %s positive words\" % len(pos_words))\n",
    "\n",
    "\n",
    "all_words = word_tokenize(all_speeches_string)\n",
    "\n",
    "all_words = [w for w in all_words if not w in stop_words]\n",
    "\n",
    "\n",
    "all_pos_words = [w for w in all_words if w in pos_words]\n",
    "\n",
    "freq_pos_words = nltk.FreqDist(all_pos_words)\n",
    "\n",
    "top_50_pos_words = [word[0] for word in freq_pos_words.most_common(50)]\n",
    "\n",
    "top_50_pos_words_string = \"\"\n",
    "\n",
    "for word in top_50_pos_words:\n",
    "    top_50_pos_words_string += (word + \" \")\n",
    "    \n",
    "\n",
    "# Create the wordcloud object\n",
    "wordcloud = WordCloud(width=960, height=960).generate(top_50_pos_words_string)\n",
    " \n",
    "# Display the generated image:\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.margins(x=0, y=0)\n",
    "plt.show()\n",
    "\n",
    "all_pos_words_1997 = [w for w in pos_words if w in speech_string_1997]\n",
    "print(len(all_pos_words_1997))\n",
    "all_pos_words_1998 = [w for w in pos_words if w in speech_string_1998]\n",
    "print(len(all_pos_words_1998))\n",
    "all_pos_words_1999 = [w for w in pos_words if w in speech_string_1999]\n",
    "print(len(all_pos_words_1999))\n",
    "all_pos_words_2000 = [w for w in pos_words if w in speech_string_2000]\n",
    "print(len(all_pos_words_2000))\n",
    "all_pos_words_2001 = [w for w in pos_words if w in speech_string_2001]\n",
    "print(len(all_pos_words_2001))\n",
    "all_pos_words_2002 = [w for w in pos_words if w in speech_string_2002]\n",
    "print(len(all_pos_words_2002))\n",
    "all_pos_words_2003 = [w for w in pos_words if w in speech_string_2003]\n",
    "print(len(all_pos_words_2003))\n",
    "all_pos_words_2004 = [w for w in pos_words if w in speech_string_2004]\n",
    "print(len(all_pos_words_2004))\n",
    "all_pos_words_2005 = [w for w in pos_words if w in speech_string_2005]\n",
    "print(len(all_pos_words_2005))\n",
    "all_pos_words_2009 = [w for w in pos_words if w in speech_string_2009]\n",
    "print(len(all_pos_words_2009))\n",
    "all_pos_words_2010 = [w for w in pos_words if w in speech_string_2010]\n",
    "print(len(all_pos_words_2010))\n",
    "all_pos_words_2011 = [w for w in pos_words if w in speech_string_2011]\n",
    "print(len(all_pos_words_2011))\n",
    "all_pos_words_2012 = [w for w in pos_words if w in speech_string_2012]\n",
    "print(len(all_pos_words_2012))\n",
    "all_pos_words_2014 = [w for w in pos_words if w in speech_string_2014]\n",
    "print(len(all_pos_words_2014))\n",
    "all_pos_words_2015 = [w for w in pos_words if w in speech_string_2015]\n",
    "print(len(all_pos_words_2015))\n",
    "all_pos_words_2016 = [w for w in pos_words if w in speech_string_2016]\n",
    "print(len(all_pos_words_2016))\n",
    "all_pos_words_2017 = [w for w in pos_words if w in speech_string_2017]\n",
    "print(len(all_pos_words_2017))\n",
    "all_pos_words_2018 = [w for w in pos_words if w in speech_string_2018]\n",
    "print(len(all_pos_words_2018))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neg_words = \"abnormal, abolish, abominable, abominably, abominate, abomination, abort, aborted, aborts, abrade, abrasive, abrupt, abruptly, abscond, absence, absent-minded, absentee, absurd, absurdity, absurdly, absurdness, abuse, abused, abuses, abusive, abysmal, abysmally, abyss, accidental, accost, accursed, accusation, accusations, accuse, accuses, accusing, accusingly, acerbate, acerbic, acerbically, ache, ached, aches, aching, acrid, acridly, acridness, acrimonious, acrimoniously, acrimony, adamant, adamantly, addict, addicted, addicting, addicts, admonish, admonisher, admonishingly, admonishment, admonition, adulterate, adulterated, adulteration, adversarial, adversary, adverse, adversity, afflict, affliction, afflictive, affront, afraid, aggravate, aggravating, aggravation, aggression, aggressive, aggressiveness, aggressor, aggrieve, aggrieved, aghast, agonies, agonize, agonizing, agonizingly, agony, aground, ail, ailing, ailment, aimless, alarm, alarmed, alarming, alarmingly, alienate, alienated, alienation, allegation, allegations, allege, allergic, allergies, allergy, aloof, altercation, ambiguity, ambiguous, ambivalence, ambivalent, ambush, amiss, amputate, anarchism, anarchist, anarchistic, anarchy, anemic, anger, angrily, angriness, angry, anguish, animosity, annihilate, annihilation, annoy, annoyance, annoyances, annoyed, annoying, annoyingly, annoys, anomalous, anomaly, antagonism, antagonist, antagonistic, antagonize, anti-, anti-occupation, anti-proliferation, anti-social, anti-us, anti-white, antipathy, antiquated, antithetical, anxieties, anxiety, anxious, anxiously, anxiousness, apathetic, apathetically, apathy, apocalypse, apocalyptic, apologist, apologists, appall, appalled, appalling, appallingly, apprehension, apprehensions, apprehensive, apprehensively, arbitrary, arcane, archaic, arduous, arduously, argumentative, arrogance, arrogant, arrogantly, ashamed, asinine, asininely, askance, asperse, aspersion, aspersions, assail, assassin, assassinate, assault, astray, asunder, atrocious, atrocities, atrocity, atrophy, attack, attacks, audacious, audaciously, audaciousness, audacity, austere, authoritarian, autocrat, autocratic, avalanche, avarice, avaricious, avariciously, avenge, averse, aversion, awful, awfully, awfulness, awkward, awkwardness, ax, babble, back-logged, back-wood, back-woods, backache, backaches, backbite, backbiting, backward, backwardness, backwoods, bad, badly, baffle, baffled, bafflement, baffling, bait, balk, banal, bane, banish, banishment, bankrupt, barbarian, barbaric, barbarically, barbarity, barbarous, barbarously, barren, baseless, bash, bashed, bashful, bashing, battered, battering, batty, bearish, beastly, bedlam, bedlamite, befoul, beg, beggar, beggarly, begging, beguile, belabor, belated, beleaguer, belie, belittle, belittled, belittling, bellicose, belligerence, belligerent, belligerently, bemoan, bemoaning, bemused, bent, berate, bereave, bereavement, bereft, berserk, beseech, beset, besiege, besmirch, betray, betrayal, betrayals, betrayer, betraying, betrays, bewail, beware, bewilder, bewildered, bewildering, bewilderingly, bewilderment, bewitch, bias, biased, biases, bicker, bickering, bid-rigging, bigotries, bigotry, biting, bitingly, bitter, bitterly, bitterness, bizarre, blab, blabber, blackmail, blah, blame, blameworthy, bland, blandish, blaspheme, blasphemous, blasphemy, blasted, blatant, blatantly, blather, bleak, bleakly, bleakness, bleed, bleeding, bleeds, blemish, blind, blinding, blindingly, blindside, blister, blistering, bloated, blockage, blockhead, bloodshed, bloodthirsty, bloody, blotchy, blow, blunder, blundering, blunders, blunt, blur, blurred, blurring, blurry, blurs, blurt, boastful, boggle, bogus, boil, boiling, boisterous, bomb, bombard, bombardment, bombastic, bondage, bonkers, bore, bored, boredom, bores, boring, botch, bother, bothered, bothering, bothers, bothersome, bowdlerize, boycott, braggart, bragger, brainless, brainwash, brash, brashly, brashness, brat, bravado, brazen, brazenly, brazenness, breach, break, break-up, break-ups, breakdown, breaking, breaks, breakup, breakups, bribery, brimstone, bristle, brittle, broke, broken, broken-hearted, brood, browbeat, bruise, bruised, bruises, bruising, brusque, brutal, brutalities, brutality, brutalize, brutalizing, brutally, brute, brutish, buckle, bug, bugging, buggy, bugs, bulkier, bulkiness, bulky,  bullâ€”-, bullies, bulls..t, bully, bullying, bullyingly, bum, bump, bumped, bumping, bumps, bumpy, bungle, bungler, bungling, bunk, burden, burdensome, burdensomely, burn, burned, burning, burns, bust, busts, busybody, butcher, butchery, buzzing, byzantine, cackle, calamities, calamitous, calamitously, calamity, callous, calumniate, calumniation, calumnies, calumnious, calumniously, calumny, cancer, cancerous, cannibal, cannibalize, capitulate, capricious, capriciously, capriciousness, capsize, careless, carelessness, caricature, carnage, carp, cartoonish, cash-strapped, castigate, castrated, casualty, cataclysm, cataclysmal, cataclysmic, cataclysmically, catastrophe, catastrophes, catastrophic, catastrophically, caustic, caustically, cautionary, cave, censure, chafe, chaff, chagrin, challenging, chaos, chaotic, chasten, chastise, chastisement, chatter, chatterbox, cheap, cheapen, cheaply, cheat, cheated, cheater, cheating, cheats, checkered, cheerless, cheesy, chide, childish, chill, chilly, chintzy, choke, choleric, choppy, chore, chronic, chunky, clamor, clamorous, clash, clique, clog, clogged, clogs, cloud, clouding, cloudy, clueless, clumsy, clunky, coarse, cocky, coerce, coercion, coercive, cold, coldly, collapse, collude, collusion, combative, combust, comical, commiserate, commonplace, commotion, commotions, complacent, complain, complained, complaining, complains, complaint, complaints, complex, complicated, complication, complicit, compulsion, compulsive, concede, conceded, conceit, conceited, concern, concerned, concerns, concession, concessions, condemn, condemnable, condemnation, condemned, condemns, condescend, condescending, condescendingly, condescension, confess, confession, confessions, confined, conflict, conflicted, conflicting, conflicts, confound, confounded, confounding, confront, confrontation, confrontational, confuse, confused, confuses, confusing, confusion, confusions, congested, congestion, cons, conservative, conspicuous, conspicuously, conspiracies, conspiracy, conspirator, conspiratorial, conspire, consternation, contagious, contaminate, contaminated, contaminates, contaminating, contamination, contempt, contemptible, contemptuous, contemptuously, contend, contention, contentious, contort, contortions, contradict, contradiction, contradictory, contrariness, contravene, contrive, contrived, controversial, controversy, convoluted, corrode, corrosion, corrosions, corrosive, corrupt, corrupted, corrupting, corruption, corrupts, costlier, costly, counter-productive, counterproductive, covetous, coward, cowardly, crabby, crack, cracked, cracks, craftily, crafty, cramp, cramped, cramping, cranky, crap, crappy, craps, crash, crashed, crashes, crashing, crass, craven, cravenly, craze, crazily, craziness, crazy, creak, creaking, creaks, credulous, creep, creeping, creeps, creepy, crept, crime, criminal, cringe, cringed, cringes, cripple, crippled, cripples, crippling, crisis, critic, critical, criticism, criticisms, criticize, criticized, criticizing, critics, cronyism, crook, crooked, crooks, crowded, crowdedness, crude, cruel, crueler, cruelest, cruelly, cruelness, cruelties, cruelty, crumble, crumbling, crummy, crumple, crumpled, crumples, crush, crushed, crushing, cry, culpable, culprit, cumbersome, curse, cursed, curses, curt, cuss, cussed, cutthroat, cynical, cynicism, damage, damaged, damages, damaging, damn, damnable, damnably, damnation, damned, damning, damper, danger, dangerous, dangerousness, dark, darken, darkened, darker, darkness, dastard, dastardly, daunt, daunting, dauntingly, dawdle, daze, dazed, dead, deadbeat, deadlock, deadly, deadweight, deaf, dearth, death, debacle, debase, debasement, debaser, debatable, debauch, debaucher, debauchery, debilitate, debilitating, debility, debt, debts, decadence, decadent, decay, decayed, deceit, deceitful, deceitfully, deceitfulness, deceive, deceiver, deceivers, deceiving, deception, deceptive, deceptively, declaim, decline, declines, declining, decrement, decrepit, decrepitude, decry, defamation, defamations, defamatory, defame, defect, defective, defects, defensive, defiance, defiant, defiantly, deficiencies, deficiency, deficient, defile, defiler, deform, deformed, defrauding, defunct, defy, degenerate, degenerately, degeneration, degradation, degrade, degrading, degradingly, dehumanization, dehumanize, deign, deject, dejected, dejectedly, dejection, delay, delayed, delaying, delays, delinquency, delinquent, delirious, delirium, delude, deluded, deluge, delusion, delusional, delusions, demean, demeaning, demise, demolish, demolisher, demon, demonic, demonize, demonized, demonizes, demonizing, demoralize, demoralizing, demoralizingly, denial, denied, denies, denigrate, denounce, dense, dent, dented, dents, denunciate, denunciation, denunciations, deny, denying, deplete, deplorable, deplorably, deplore, deploring, deploringly, deprave, depraved, depravedly, deprecate, depress, depressed, depressing, depressingly, depression, depressions, deprive, deprived, deride, derision, derisive, derisively, derisiveness, derogatory, desecrate, desert, desertion, desiccate, desiccated, desolate, desolately, desolation, despair, despairing, despairingly, desperate, desperately, desperation, despicable, despicably, despise, despised, despoil, despoiler, despondence, despondency, despondent, despondently, despot, despotic, despotism, destitute, destitution, destroy, destroyer, destruction, destructive, desultory, deter, deteriorate, deteriorating, deterioration, deterrent, detest, detestable, detestably, detested, detesting, detests, detract, detracted, detracting, detraction, detracts, detriment, detrimental, devastate, devastated, devastates, devastating, devastatingly, devastation, deviate, deviation, devil, devilish, devilishly, devilment, devilry, devious, deviously, deviousness, devoid, diabolic, diabolical, diabolically, diametrically, diatribe, diatribes, dick, dictator, dictatorial, die, die-hard, died, dies, difficult, difficulties, difficulty, diffidence, dilapidated, dilemma, dilly-dally, dim, dimmer, ding, dings, dinky, dire, direly, direness, dirt, dirty, disable, disabled, disaccord, disadvantage, disadvantaged, disadvantageous, disadvantages, disaffect, disaffected, disaffirm, disagree, disagreeable, disagreeably, disagreed, disagreeing, disagreement, disagrees, disallow, disappoint, disappointed, disappointing, disappointingly, disappointment, disappointments, disappoints, disapprobation, disapproval, disapprove, disapproving, disarm, disarray, disaster, disastrous, disastrously, disavow, disavowal, disbelief, disbelieve, disbeliever, disclaim, discombobulate, discomfit, discomfort, discompose, disconcert, disconcerted, disconcerting, disconcertingly, disconsolate, disconsolately, disconsolation, discontent, discontented, discontentedly, discontinued, discontinuity, discontinuous, discord, discordance, discordant, discountenance, discourage, discouragement, discouraging, discouragingly, discourteous, discourteously, discredit, discrepant, discriminate, discrimination, discriminatory, disdain, disdained, disdainful, disdainfully, disfavor, disgrace, disgraced, disgraceful, disgracefully, disgruntle, disgruntled, disgust, disgusted, disgustedly, disgustful, disgustfully, disgusting, disgustingly, dishearten, disheartening, dishearteningly, dishonest, dishonestly, dishonesty, dishonor, dishonorable, disillusion, disillusioned, disillusionment, disillusions, disinclination, disinclined, disingenuous, disingenuously, disintegrate, disintegrated, disintegrates, disintegration, disinterest, disinterested, dislike, disliked, dislikes, disliking, dislocated, disloyal, disloyalty, dismal, dismally, dismalness, dismay, dismayed, dismaying, dismayingly, dismissive, dismissively, disobedience, disobedient, disobey, disorder, disordered, disorderly, disorganized, disorient, disoriented, disown, disparage, disparaging, disparagingly, dispensable, dispirit, dispirited, dispiritedly, dispiriting, displace, displaced, displease, displeased, displeasing, displeasure, disproportionate, disprove, disputable, dispute, disputed, disquiet, disquieting, disquietingly, disquietude, disregard, disregardful, disreputable, disrepute, disrespect, disrespectable, disrespectful, disrespectfully, disrespectfulness, disrespecting, disrupt, disruption, disruptive, dissatisfaction, dissatisfactory, dissatisfied, dissatisfies, dissatisfy, dissatisfying, dissed, dissemble, dissembler, dissension, dissent, dissenter, dissention, disservice, disses, dissidence, dissident, dissidents, dissing, dissocial, dissolute, dissolution, dissonance, dissonant, dissonantly, dissuade, dissuasive, distains, distaste, distasteful, distastefully, distort, distorted, distortion, distorts, distract, distracting, distraction, distraught, distraughtly, distress, distressed, distressing, distressingly, distrust, distrustful, distrusting, disturb, disturbance, disturbed, disturbing, disturbingly, disunity, disvalue, divergent, divisive, divisively, divisiveness, dizzy, doddering, dogged, doggedly, dogmatic, doldrums, domineer, domineering, doom, doomed, doomsday, dope, doubt, doubtful, doubtfully, doubts, douchebag, douchebags, downbeat, downcast, downer, downfall, downfallen, downgrade, downhearted, downheartedly, downhill, downside, downsides, downturn, downturns, drab, draconian, draconic, drag, dragged, dragging, dragoon, drags, drain, drained, draining, drains, drastic, drastically, drawback, drawbacks, dread, dreadful, dreadfully, dreadfulness, dreary, dripped, dripping, drippy, drips, drones, droop, droops, drop-out, drop-outs, dropout, dropouts, drought, drowning, drunk, drunkard, drunken, dubious, dubiously, dubitable, dud, dull, dullard, dumb, dumbfound, dump, dumped, dumping, dumps, dunce, dungeon, dungeons, dupe, dust, dusty, dwindling, dying, earsplitting, eccentric, eccentricity, effigy, effrontery, egocentric, egomania, egotism, egotistical, egotistically, egregious, egregiously, election-rigger, elimination, emaciated, emasculate, embarrass, embarrassing, embarrassingly, embarrassment, embattled, embroil, embroiled, embroilment, emergency, emphatic, emphatically, emptiness, encroach, encroachment, endanger, enemies, enemy, enervate, enfeeble, enflame, engulf, enjoin, enmity, enrage, enraged, enraging, enslave, entangle, entanglement, entrap, entrapment, envious, enviously, enviousness, epidemic, equivocal, erase, erode, erodes, erosion, err, errant, erratic, erratically, erroneous, erroneously, error, errors, eruptions, escapade, eschew, estranged, evade, evasion, evasive, evil, evildoer, evils, eviscerate, exacerbate, exaggerate, exaggeration, exasperate, exasperated, exasperating, exasperatingly, exasperation, excessive, excessively, exclusion, excoriate, excruciating, excruciatingly, excuse, excuses, execrate, exhaust, exhausted, exhaustion, exhausts, exhort, exile, exorbitant, exorbitantly, expel, expensive, expire, expired, explode, exploit, exploitation, explosive, expropriate, expropriation, expulse, expunge, exterminate, extermination, extinguish, extort, extortion, extraneous, extravagance, extravagant, extravagantly, extremism, extremist, extremists, eyesore, fabricate, fabrication, facetious, facetiously, fail, failed, failing, fails, failure, failures, faint, fainthearted, faithless, fake, fall, fallacies, fallacious, fallaciously, fallaciousness, fallacy, fallen, falling, fallout, falls, FALSE, falsehood, falsely, falsify, falter, faltered, famine, famished, fanatic, fanatical, fanatically, fanaticism, fanatics, fanciful, far-fetched, farce, farcical, farcical-yet-provocative, farcically, farfetched, fascism, fascist, fastidious, fastidiously, fat, fat-cat, fat-cats, fatal, fatalistic, fatalistically, fatally, fateful, fatefully, fathomless, fatigue, fatigued, fatty, fatuity, fatuous, fatuously, fault, faults, faulty, fawningly, faze, fear, fearful, fearfully, fears, fearsome, feckless, feeble, feebleminded, feign, feint, fell, felon, felonious, ferociously, ferocity, fetid, fever, feverish, fevers, fiasco, fib, fibber, fickle, fiction, fictional, fictitious, fidget, fidgety, fiend, fiendish, fierce, figurehead, filth, filthy, finagle, finicky, fissures, fist, flabbergast, flabbergasted, flagging, flagrant, flagrantly, flair, flairs, flak, flake, flakey, flaking, flaky, flare, flares, flat-out, flaunt, flaw, flawed, flaws, flee, fleeing, fleer, flees, fleeting, flicker, flickering, flickers, flighty, flimflam, flimsy, flirt, flirty, floored, flounder, floundering, flout, fluster, foe, fool, fooled, foolhardy, foolish, foolishly, foolishness, forbid, forbidden, forbidding, forceful, foreboding, forebodingly, forfeit, forged, forgetful, forgetfully, forgetfulness, forlorn, forlornly, forsake, forsaken, forswear, foul, foully, foulness, fractious, fractiously, fracture, fragile, fragmented, frail, frantic, frantically, franticly, fraud, fraudulent, fraught, frazzle, frazzled, freak, freaking, freakish, freakishly, freaks, freeze, freezes, freezing, frenetic, frenetically, frenzied, frenzy, fret, fretful, frets, friction, frictions, fried, frigging, fright, frighten, frightening, frighteningly, frightful, frightfully, frigid, frost, frown, froze, frozen, fruitless, fruitlessly, frustrate, frustrated, frustrates, frustrating, frustratingly, frustration, frustrations, fudge, fugitive, full-blown, fulminate, fumble, fume, fumes, fundamentalism, funky, funnily, funny, furious, furiously, furor, fury, fuss, fussy, fustigate, fusty, futile, futilely, futility, fuzzy, gabble, gaff, gaffe, gainsay, gainsayer, gall, galling, gallingly, galls, gangster, gape, garbage, garish, gasp, gauche, gaudy, gawk, gawky, geezer, genocide, get-rich, ghastly, ghetto, ghosting, gibber, gibberish, gibe, giddy, gimmick, gimmicked, gimmicking, gimmicks, gimmicky, glare, glaringly, glib, glibly, glitch, glitches, gloatingly, gloom, gloomy, glower, glum, glut, gnawing, goad, goading, god-awful, goof, goofy, goon, gossip, graceless, gracelessly, graft, grainy, grapple, grate, grating, gravely, greasy, greed, greedy, grief, grievance, grievances, grieve, grieving, grievous, grievously, grim, grimace, grind, gripe, gripes, grisly, gritty, gross, grossly, grotesque, grouch, grouchy, groundless, grouse, growl, grudge, grudges, grudging, grudgingly, gruesome, gruesomely, gruff, grumble, grumpier, grumpiest, grumpily, grumpy, guile, guilt, guiltily, guilty, gullible, gutless, gutter, hack, hacks, haggard, haggle, halfhearted, halfheartedly, hallucinate, hallucination, hamper, hampered, handicapped, hang, hangs, haphazard, hapless, harangue, harass, harassed, harasses, harassment, harboring, harbors, hard, hard-hit, hard-liner, hardball, harden, hardened, hardheaded, hardhearted, hardliner, hardliners, hardship, hardships, harm, harmed, harmful, harms, harpy, harridan, harried, harrow, harsh, harshly, hassle, hassled, hassles, haste, hastily, hasty, hate, hated, hateful, hatefully, hatefulness, hater, haters, hates, hating, hatred, haughtily, haughty, haunt, haunting, havoc, hawkish, haywire, hazard, hazardous, haze, hazy, head-aches, headache, headaches, heartbreaker, heartbreaking, heartbreakingly, heartless, heathen, heavy-handed, heavyhearted, heck, heckle, heckled, heckles, hectic, hedge, hedonistic, heedless, hefty, hegemony, heinous, hell, hell-bent, hellion, hells, helpless, helplessly, helplessness, heresy, heretic, heretical, hesitant, hideous, hideously, hideousness, high-priced, hinder, hindrance, hiss, hissed, hissing, ho-hum, hoard, hoax, hobble, hogs, hollow, hoodwink, hooligan, hopeless, hopelessly, hopelessness, horde, horrendous, horrendously, horrible, horrid, horrific, horrified, horrifies, horrify, horrifying, hostage, hostile, hostilities, hostility, hotbeds, hothead, hotheaded, hothouse, hubris, huckster, hum, humid, humiliate, humiliating, humiliation, humming, hung, hurt, hurtful, hurting, hurts, hustler, hype, hypocrisy, hypocrite, hypocrites, hypocritical, hypocritically, hysteria, hysteric, hysterical, hysterically, hysterics, idiocies, idiocy, idiot, idiotic, idiotically, idiots, idle, ignoble, ignominious, ignominiously, ignominy, ignorance, ignorant, ignore, ill-advised, ill-conceived, ill-defined, ill-designed, ill-fated, ill-favored, ill-formed, ill-mannered, ill-natured, ill-sorted, ill-tempered, ill-treated, ill-treatment, ill-usage, ill-used, illegal, illegally, illegitimate, illicit, illiterate, illness, illogical, illogically, illusion, illusions, illusory, imaginary, imbalance, imbecile, imbroglio, immaterial, immature, imminence, imminently, immobilized, immoderate, immoderately, immodest, immoral, immorality, immorally, immovable, impair, impaired, impasse, impatience, impatient, impatiently, impeach, impedance, impede, impediment, impending, impenitent, imperfect, imperfection, imperfections, imperfectly, imperialist, imperil, imperious, imperiously, impermissible, impersonal, impertinent, impetuous, impetuously, impiety, impinge, impious, implacable, implausible, implausibly, implicate, implication, implode, impolite, impolitely, impolitic, importunate, importune, impose, imposers, imposing, imposition, impossible, impossibly, impotent, impoverish, impoverished, impractical, imprecate, imprecise, imprecisely, imprecision, imprison, imprisonment, improbability, improbable, improbably, improper, improperly, impropriety, imprudence, imprudent, impudence, impudent, impudently, impugn, impulsive, impulsively, impunity, impure, impurity, inability, inaccuracies, inaccuracy, inaccurate, inaccurately, inaction, inactive, inadequacy, inadequate, inadequately, inadvisable, inane, inanely, inappropriate, inappropriately, inapt, inarticulate, inattentive, inaudible, incapable, incapably, incautious, incendiary, incense, incessant, incessantly, incite, incitement, incivility, inclement, incoherence, incoherent, incoherently, incommensurate, incomparable, incomparably, incompatibility, incompatible, incompetence, incompetent, incompetently, incomplete, incomprehensible, incomprehension, inconceivable, inconceivably, incongruous, incongruously, inconsequential, inconsequentially, inconsiderate, inconsiderately, inconsistencies, inconsistency, inconsistent, inconsolable, inconsolably, inconstant, inconvenience, inconveniently, incorrect, incorrectly, incorrigible, incorrigibly, incredulous, incredulously, inculcate, indecency, indecent, indecently, indecision, indecisive, indecisively, indefensible, indelicate, indeterminable, indeterminably, indeterminate, indifference, indifferent, indigent, indignant, indignantly, indignation, indignity, indiscernible, indiscreet, indiscreetly, indiscretion, indiscriminate, indiscriminately, indistinguishable, indoctrinate, indoctrination, indolent, indulge, ineffective, ineffectively, ineffectiveness, ineffectual, ineffectually, inefficacy, inefficiency, inefficient, inefficiently, inelegance, inelegant, ineligible, inept, ineptitude, ineptly, inequalities, inequality, inequitable, inequitably, inequities, inescapable, inescapably, inessential, inevitable, inevitably, inexcusable, inexcusably, inexorable, inexorably, inexperience, inexperienced, inexpert, inexpertly, inexpiable, inextricable, inextricably, infamous, infamously, infamy, infected, infection, infections, inferior, inferiority, infernal, infest, infested, infidel, infidels, infiltrator, infiltrators, infirm, inflame, inflammation, inflammatory, inflated, inflationary, inflexible, inflict, infraction, infringe, infringement, infringements, infuriate, infuriated, infuriating, infuriatingly, inglorious, ingrate, ingratitude, inhibit, inhibition, inhospitable, inhuman, inhumane, inhumanity, inimical, inimically, iniquitous, iniquity, injudicious, injure, injurious, injury, injustice, injustices, innuendo, inoperable, inopportune, inordinate, inordinately, insane, insanely, insanity, insatiable, insecure, insecurity, insensible, insensitive, insensitively, insensitivity, insidious, insidiously, insignificance, insignificant, insignificantly, insincere, insincerely, insincerity, insinuate, insinuating, insinuation, insolence, insolent, insolently, insolvent, insouciance, instability, instigate, instigator, instigators, insubordinate, insubstantial, insubstantially, insufferable, insufferably, insufficiency, insufficient, insufficiently, insular, insult, insulted, insulting, insultingly, insults, insupportable, insurmountable, insurmountably, insurrection, intense, interfere, interference, interferes, intermittent, interrupt, interruption, interruptions, intimidate, intimidating, intimidatingly, intimidation, intolerable, intolerance, intoxicate, intractable, intransigence, intransigent, intrude, intrusion, intrusive, inundate, inundated, invader, invalid, invalidate, invalidity, invasive, invective, inveigle, invidious, invidiously, invidiousness, invisible, involuntarily, involuntary, irascible, irate, irately, ire, irk, irked, irking, irks, irksome, irksomely, irksomeness, ironic, ironical, ironically, ironies, irony, irrational, irrationality, irrationally, irrationals, irreconcilable, irrecoverable, irrecoverably, irredeemable, irredeemably, irregular, irregularity, irrelevance, irrelevant, irreparable, irrepressible, irresolute, irresponsible, irresponsibly, irretrievable, irreversible, irritable, irritably, irritant, irritate, irritated, irritating, irritation, irritations, isolate, isolated, isolation, issue, issues, itch, itching, itchy, jabber, jaded, jagged, jam, jarring, jaundiced, jealous, jealously, jealousness, jealousy, jeer, jeering, jeeringly, jeers, jeopardize, jeopardy, jerk, jerky, jitter, jitters, jittery, job-killing, jobless, joke, joker, jolt, judder, juddering, judders, jumpy, junk, junky, junkyard, kill, killed, killer, killing, killjoy, kills, knave, knife, knock, knotted, kook, kooky, lack, lackadaisical, lacked, lackey, lackeys, lacking, lackluster, lacks, laconic, lag, lagged, lagging, lags, laid-off, lambast, lambaste, lame, lame-duck, lament, lamentable, lamentably, languid, languish, languor, languorous, languorously, lanky, lapse, lapsed, lapses, lascivious, last-ditch, latency, laughable, laughably, laughingstock, lawbreaker, lawbreaking, lawless, lawlessness, layoff, layoff-happy, lazy, leak, leakage, leakages, leaking, leaks, leaky, lecher, lecherous, lechery, leech, leer, leery, left-leaning, lemon, lengthy, less-developed, lesser-known, letch, lethal, lethargic, lethargy, lewd, lewdly, lewdness, liability, liable, liar, liars, licentious, licentiously, licentiousness, lie, lied, lies, life-threatening, lifeless, limit, limitation, limitations, limited, limits, limp, listless, litigious, little-known, livid, lividly, loath, loathe, loathing, loathly, loathsome, loathsomely, lone, loneliness, lonely, loner, lonesome, long-time, long-winded, longing, longingly, loophole, loopholes, loose, loot, lose, loser, losers, loses, losing, loss, losses, lost, loud, louder, lousy, loveless, lovelorn, low-rated, lowly, ludicrous, ludicrously, lugubrious, lukewarm, lull, lumpy, lunatic, lurch, lure, lurid, lurk, lurking, lying, macabre, mad, madden, maddening, maddeningly, madder, madly, madman, madness, maladjusted, maladjustment, malady, malaise, malcontent, malcontented, maledict, malevolence, malevolent, malevolently, malice, malicious, maliciously, maliciousness, malign, malignant, malodorous, maltreatment, mangle, mangled, mangles, mangling, mania, maniac, maniacal, manic, manipulate, manipulation, manipulative, manipulators, mar, marginal, marginally, martyrdom, martyrdom-seeking, mashed, massacre, massacres, matte, mawkish, mawkishly, mawkishness, meager, meaningless, meanness, measly, meddle, meddlesome, mediocre, mediocrity, melancholy, melodramatic, melodramatically, meltdown, menace, menacing, menacingly, mendacious, mendacity, menial, merciless, mercilessly, mess, messed, messes, messing, messy, midget, miff, militancy, mindless, mindlessly, mirage, mire, misalign, misaligned, misaligns, misapprehend, misbecome, misbecoming, misbegotten, misbehave, misbehavior, miscalculate, miscalculation, miscellaneous, mischief, mischievous, mischievously, misconception, misconceptions, miscreant, miscreants, misdirection, miser, miserable, miserableness, miserably, miseries, miserly, misery, misfit, misfortune, misgiving, misgivings, misguidance, misguide, misguided, mishandle, mishap, misinform, misinformed, misinterpret, misjudge, misjudgment, mislead, misleading, misleadingly, mismanage, mispronounce, mispronounced, mispronounces, misread, misreading, misrepresent, misrepresentation, miss, missed, misses, misstatement, mist, mistake, mistaken, mistakenly, mistakes, mistress, mistrust, mistrustful, mistrustfully, mists, misunderstand, misunderstanding, misunderstandings, misunderstood, misuse, moan, mobster, mock, mocked, mockeries, mockery, mocking, mockingly, mocks, molest, molestation, monotonous, monotony, monster, monstrosities, monstrosity, monstrous, monstrously, moody, moot, mope, morbid, morbidly, mordant, mordantly, moribund, moron, moronic, morons, mortification, mortified, mortify, mortifying, motionless, motley, mourn, mourner, mournful, mournfully, muddle, muddy, mudslinger, mudslinging, mulish, multi-polarization, mundane, murder, murderer, murderous, murderously, murky, muscle-flexing, mushy, musty, mysterious, mysteriously, mystery, mystify, myth, nag, nagging, naive, naively, narrower, nastily, nastiness, nasty, naughty, nauseate, nauseates, nauseating, nauseatingly, naÃ¯ve, nebulous, nebulously, needless, needlessly, needy, nefarious, nefariously, negate, negation, negative, negatives, negativity, neglect, neglected, negligence, negligent, nemesis, nepotism, nervous, nervously, nervousness, nettle, nettlesome, neurotic, neurotically, niggle, niggles, nightmare, nightmarish, nightmarishly, nitpick, nitpicking, noise, noises, noisier, noisy, non-confidence, nonexistent, nonresponsive, nonsense, nosey, notoriety, notorious, notoriously, noxious, nuisance, numb, obese, object, objection, objectionable, objections, oblique, obliterate, obliterated, oblivious, obnoxious, obnoxiously, obscene, obscenely, obscenity, obscure, obscured, obscures, obscurity, obsess, obsessive, obsessively, obsessiveness, obsolete, obstacle, obstinate, obstinately, obstruct, obstructed, obstructing, obstruction, obstructs, obtrusive, obtuse, occlude, occluded, occludes, occluding, odd, odder, oddest, oddities, oddity, oddly, odor, offence, offend, offender, offending, offenses, offensive, offensively, offensiveness, officious, ominous, ominously, omission, omit, one-sided, onerous, onerously, onslaught, opinionated, opponent, opportunistic, oppose, opposition, oppositions, oppress, oppression, oppressive, oppressively, oppressiveness, oppressors, ordeal, orphan, ostracize, outbreak, outburst, outbursts, outcast, outcry, outlaw, outmoded, outrage, outraged, outrageous, outrageously, outrageousness, outrages, outsider, over-acted, over-awe, over-balanced, over-hyped, over-priced, over-valuation, overact, overacted, overawe, overbalance, overbalanced, overbearing, overbearingly, overblown, overdo, overdone, overdue, overemphasize, overheat, overkill, overloaded, overlook, overpaid, overplay, overpower, overpriced, overrated, overreach, overrun, overshadow, oversight, oversights, oversimplification, oversimplified, oversimplify, oversize, overstate, overstated, overstatement, overstatements, overstates, overtaxed, overthrow, overthrows, overturn, overweight, overwhelm, overwhelmed, overwhelming, overwhelmingly, overwhelms, overzealous, overzealously, pain, painful, painfully, pains, pale, pales, paltry, pan, pandemonium, pander, pandering, panders, panic, panicked, panicking, panicky, paradoxical, paradoxically, paralyzed, paranoia, paranoid, parasite, pariah, parody, partiality, partisan, partisans, passive, passiveness, pathetic, pathetically, patronize, paucity, pauper, paupers, payback, peculiar, peculiarly, pedantic, peeled, peeve, peeved, peevish, peevishly, penalize, penalty, perfidious, perfunctory, peril, perilous, perilously, perish, pernicious, perplex, perplexed, perplexing, perplexity, persecute, persecution, pertinacious, pertinaciously, pertinacity, perturb, perturbed, pervasive, perverse, perversely, perversion, perversity, pervert, perverted, perverts, pessimism, pessimistic, pessimistically, pest, pestilent, petrified, petrify, pettifog, petty, phobia, phobic, phony, picket, picketed, picketing, pickets, picky, pig, pigs, pillage, pillory, pimple, pinch, pique, pitiable, pitiful, pitifully, pitiless, pitilessly, pittance, pity, plagiarize, plague, plaything, plea, pleas, plebeian, plight, plot, plotters, ploy, plunder, plunderer, pointless, pointlessly, poison, poisonous, poisonously, pokey, poky, pollute, polluter, polluters, pompous, poor, poorer, poorest, poorly, posturing, pout, poverty, powerless, prate, pratfall, prattle, precarious, precariously, precipitate, precipitous, predatory, predicament, prejudge, prejudice, prejudices, prejudicial, premeditated, preoccupy, preposterous, preposterously, presumptuous, presumptuously, pretend, pretense, pretentious, pretentiously, prevaricate, pricey, pricier, prick, prickle, prickles, prideful, primitive, prison, prisoner, problem, problematic, problems, procrastinate, procrastinates, procrastination, profane, profanity, prohibit, prohibitive, prohibitively, propaganda, propagandize, proprietary, prosecute, protest, protested, protesting, protests, protracted, provocation, provocative, provoke, pry, pugnacious, pugnaciously, pugnacity, punch, punish, punishable, punitive, punk, puny, puppet, puppets, puzzled, puzzlement, puzzling, quack, qualm, qualms, quandary, quarrel, quarrels, quarrelsome, quash, queer, questionable, quibble, quibbles, quitter, rabid, racism, racist, racists, racy, radical, radicalization, radically, radicals, rage, ragged, raging, rail, raked, rampage, rampant, ramshackle, rancor, randomly, rankle, rant, ranted, ranting, rants, rape, raped, raping, rascal, rascals, rash, rattle, rattled, rattles, ravage, raving, reactionary, rebellious, rebuff, rebuke, recalcitrant, recant, recession, recessionary, reckless, recklessly, recklessness, recoil, recourses, redundancy, redundant, refusal, refuse, refused, refuses, refusing, refutation, refute, refuted, refutes, refuting, regress, regression, regressive, regret, regretful, regretfully, regrets, regrettable, regrettably, regretted, reject, rejected, rejecting, rejection, rejects, relapse, relentless, relentlessly, relentlessness, reluctance, reluctant, reluctantly, remorse, remorseful, remorsefully, remorseless, remorselessly, remorselessness, renounce, renunciation, repel, repetitive, reprehensible, reprehensibly, reprehension, reprehensive, repress, repression, repressive, reprimand, reproach, reproachful, reprove, reprovingly, repudiate, repudiation, repugnance, repugnant, repugnantly, repulse, repulsed, repulsing, repulsive, repulsively, repulsiveness, resent, resentful, resentment, resignation, resigned, resistance, restless, restlessness, restrict, restricted, restriction, restrictive, resurgent, retaliate, retaliatory, reticent, retract, retreat, retreated, revenge, revengeful, revengefully, revert, revile, reviled, revoke, revolt, revolting, revoltingly, revulsion, revulsive, rhapsodize, rhetoric, rhetorical, ricer, ridicule, ridicules, ridiculous, ridiculously, rife, rift, rifts, rigid, rigidity, rigidness, rile, riled, rip, rip-off, ripped, risk, risks, risky, rival, rivalry, roadblocks, rocky, rogue, rollercoaster, rot, rotten, rough, rubbish, rude, rue, ruffian, ruffle, ruin, ruined, ruining, ruinous, ruins, rumbling, rumor, rumors, rumple, run-down, runaway, rupture, rust, rusts, rusty, rut, ruthless, ruthlessly, ruthlessness, ruts, sabotage, sack, sacrificed, sad, sadden, sadly, sadness, sag, sagged, sagging, saggy, sags, salacious, sanctimonious, sap, sarcasm, sarcastic, sarcastically, sardonic, sardonically, sass, satirical, satirize, savage, savaged, savagery, savages, scaly, scam, scams, scandal, scandalize, scandalized, scandalous, scandalously, scandals, scant, scapegoat, scar, scarce, scarcely, scarcity, scare, scared, scarier, scariest, scarily, scarred, scars, scary, scathing, scathingly, scoff, scold, scolded, scolding, scorching, scorn, scornful, scornfully, scoundrel, scourge, scowl, scramble, scrambled, scrambles, scrambling, scrap, scratch, scratched, scratches, scratchy, scream, screech, screw-up, screwed, screwed-up, screwy, scuff, scuffs, scum, scummy, second-class, second-tier, secretive, sedentary, seedy, seethe, seething, self-coup, self-criticism, self-defeating, self-destructive, self-humiliation, self-interest, self-interested, self-serving, selfish, selfishly, selfishness, senile, sensationalize, senseless, senselessly, seriousness, sermonize, servitude, set-up, setback, setbacks, sever, severe, severity, shabby, shadowy, shady, shake, shaky, shallow, sham, shambles, shame, shameful, shamefully, shamefulness, shameless, shamelessly, shamelessness, shark, sharply, shatter, shimmer, shimmy, shipwreck, shirk, shirker, shiver, shock, shocked, shocking, shockingly, shoddy, short-lived, shortage, shortchange, shortcoming, shortcomings, shortness, shortsighted, shortsightedness, showdown, shrew, shriek, shrill, shrilly, shrivel, shroud, shrouded, shrug, shun, shunned, sick, sicken, sickening, sickeningly, sickly, sickness, sidetrack, sidetracked, siege, silly, simplistic, simplistically, sin, sinful, sinfully, sinister, sinisterly, sink, sinking, skeletons, skeptic, skeptical, skeptically, skepticism, sketchy, skimpy, skinny, skittish, skittishly, skulk, slack, slander, slanderer, slanderous, slanderously, slanders, slap, slashing, slaughter, slaughtered, slave, slaves, sleazy, slime, slog, slogged, slogging, slogs, sloppily, sloppy, sloth, slothful, slow, slow-moving, slowed, slower, slowest, slowly, slug, sluggish, slump, slumping, slur, sly, smack, smallish, smash, smear, smell, smelled, smelling, smells, smelly, smelt, smoke, smokescreen, smolder, smoldering, smother, smudge, smudged, smudges, smudging, smug, smugly, snag, snagged, snagging, snags, snappish, snappishly, snare, snarky, snarl, sneak, sneakily, sneaky, sneer, sneering, sneeringly, snob, snobbish, snobby, snobs, snub, soapy, sob, sober, sobering, solemn, solicitude, somber, sore, sorely, soreness, sorrow, sorrowful, sorrowfully, sorry, sour, sourly, spade, spank, spew, spewed, spewing, spews, spilling, spinster, spiritless, spite, spiteful, spitefully, spitefulness, splatter, split, splitting, spoil, spoilage, spoilages, spoiled, spoils, spook, spookier, spookiest, spookily, spooky, spoon-fed, spoon-feed, sporadic, spotty, spurious, spurn, sputter, squabble, squabbling, squander, squash, squeak, squeaks, squeaky, squeal, squealing, squeals, squirm, stab, stagnant, stagnate, stagnation, staid, stain, stains, stale, stalemate, stall, stalls, stammer, stampede, standstill, stark, starkly, startle, startling, startlingly, starvation, starve, static, steal, stealing, steals, steep, steeply, stench, stereotype, stereotypical, stereotypically, stern, stew, sticky, stiff, stiffness, stifle, stifling, stiflingly, stigma, stigmatize, sting, stinging, stingingly, stingy, stink, stinks, stodgy, stole, stolen, stooge, stooges, stormy, straggle, straggler, strain, strained, straining, strange, strangely, stranger, strangest, strangle, streaky, strenuous, stress, stresses, stressful, stressfully, stricken, strict, strictly, strident, stridently, strife, strike, stringent, stringently, struck, struggle, struggled, struggles, struggling, strut, stubborn, stubbornly, stubbornness, stuck, stuffy, stumble, stumbled, stumbles, stump, stumped, stumps, stun, stunt, stunted, stupid, stupidest, stupidity, stupidly, stupor, stutter, stuttered, stuttering, stutters, sty, stymied, sub-par, subdued, subjected, subjection, subjugate, subjugation, submissive, subordinate, subpoena, subpoenas, subservience, subservient, substandard, subtract, subversion, subversive, subversively, subvert, succumb, suck, sucked, sucker, sucks, sucky, sue, sued, sues, suffer, suffered, sufferer, sufferers, suffering, suffers, suffocate, sugar-coat, sugar-coated, sugarcoated, suicidal, suicide, sulk, sullen, sully, sunder, sunk, sunken, superficial, superficiality, superficially, superfluous, superstition, superstitious, suppress, suppression, surrender, susceptible, suspect, suspicion, suspicions, suspicious, suspiciously, swagger, swamped, sweaty, swelled, swelling, swindle, swipe, swollen, symptom, symptoms, syndrome, taboo, tacky, taint, tainted, tamper, tangle, tangled, tangles, tank, tanked, tanks, tantrum, tardy, tarnish, tarnished, tarnishes, tarnishing, tattered, taunt, taunting, tauntingly, taunts, taut, tawdry, taxing, tease, teasingly, tedious, tediously, temerity, temper, tempest, temptation, tenderness, tense, tension, tentative, tentatively, tenuous, tenuously, tepid, terrible, terribleness, terribly, terror, terror-genic, terrorism, terrorize, testily, testy, tetchily, tetchy, thankless, thicker, thirst, thorny, thoughtless, thoughtlessly, thoughtlessness, thrash, threat, threaten, threatening, threats, threesome, throb, throbbed, throbbing, throbs, throttle, thug, thumb-down, thumbs-down, thwart, time-consuming, timid, timidity, timidly, tin-y, tingled, tingling, tired, tiresome, tiring, tiringly, toil, toll, top-heavy, topple, torment, tormented, torrent, tortuous, torture, tortured, tortures, torturing, torturous, torturously, totalitarian, touchy, toughness, tout, touted, touts, toxic, traduce, tragedy, tragic, tragically, traitor, traitorous, traitorously, tramp, trample, transgress, transgression, trap, trapped, trash, trashed, trashy, trauma, traumatic, traumatically, traumatize, traumatized, travesties, travesty, treacherous, treacherously, treachery, treason, treasonous, trick, tricked, trickery, tricky, trivial, trivialize, trouble, troubled, troublemaker, troubles, troublesome, troublesomely, troubling, troublingly, truant, tumble, tumbled, tumbles, tumultuous, turbulent, turmoil, twist, twisted, twists, two-faced, two-faces, tyrannical, tyrannically, tyranny, tyrant, ugh, uglier, ugliest, ugliness, ugly, ulterior, ultimatum, ultimatums, ultra-hardline, un-viewable, unable, unacceptable, unacceptably, unaccustomed, unachievable, unaffordable, unappealing, unattractive, unauthentic, unavailable, unavoidably, unbearable, unbelievable, unbelievably, uncaring, uncertain, uncivil, uncivilized, unclean, unclear, uncollectible, uncomfortable, uncomfortably, uncompetitive, uncompromising, uncompromisingly, unconfirmed, unconstitutional, uncontrolled, unconvincing, unconvincingly, uncooperative, uncouth, uncreative, undecided, undefined, undependability, undependable, undercut, undercuts, undercutting, underdog, underestimate, underlings, undermine, undermined, undermines, undermining, underpaid, underpowered, undersized, undesirable, undetermined, undid, undignified, undissolved, undocumented, undone, undue, unease, uneasily, uneasiness, uneasy, uneconomical, unemployed, unequal, unethical, uneven, uneventful, unexpected, unexpectedly, unexplained, unfairly, unfaithful, unfaithfully, unfamiliar, unfavorable, unfeeling, unfinished, unfit, unforeseen, unforgiving, unfortunate, unfortunately, unfounded, unfriendly, unfulfilled, unfunded, ungovernable, ungrateful, unhappily, unhappiness, unhappy, unhealthy, unhelpful, unilateralism, unimaginable, unimaginably, unimportant, uninformed, uninsured, unintelligible, unipolar, unjust, unjustifiable, unjustifiably, unjustified, unjustly, unkind, unkindly, unknown, unlawful, unlawfully, unlawfulness, unleash, unlicensed, unlikely, unlucky, unmoved, unnatural, unnaturally, unnecessary, unneeded, unnerve, unnerved, unnerving, unnervingly, unnoticed, unobserved, unorthodox, unorthodoxy, unpleasant, unpopular, unpredictable, unprepared, unproductive, unprofitable, unproved, unproven, unqualified, unravel, unraveled, unreachable, unreadable, unrealistic, unreasonable, unreasonably, unrelenting, unrelentingly, unreliability, unreliable, unresolved, unresponsive, unrest, unruly, unsafe, unsatisfactory, unsavory, unscrupulous, unscrupulously, unsecure, unseemly, unsettle, unsettled, unsettling, unsettlingly, unskilled, unsophisticated, unsound, unspeakable, unspecified, unstable, unsteadily, unsteadiness, unsteady, unsuccessful, unsuccessfully, unsupported, unsupportive, unsure, unsuspecting, unsustainable, untenable, untested, unthinkable, unthinkably, untimely, untouched, untrue, untrustworthy, untruthful, unusable, unusably, unusual, unusually, unwanted, unwarranted, unwatchable, unwelcome, unwell, unwieldy, unwilling, unwillingly, unwillingness, unwise, unwisely, unworkable, unworthy, unyielding, upbraid, upheaval, uprising, uproar, uproarious, uproariously, uproot, upset, upsets, upsetting, upsettingly, urgent, useless, usurp, usurper, utterly, vagrant, vague, vagueness, vain, vainly, vanity, vehement, vehemently, vengeance, vengeful, vengefully, vengefulness, venom, venomous, venomously, vent, vestiges, vex, vexation, vexing, vexingly, vibrate, vibrated, vibrates, vibrating, vibration, vice, vicious, viciously, viciousness, victimize, vile, vileness, vilify, villainous, villainously, villains, vindictive, vindictively, vindictiveness, violate, violation, violator, violators, violent, violently, viper, virulence, virulent, virulently, virus, vociferous, vociferously, volatile, volatility, vomit, vomited, vomiting, vomits, vulgar, vulnerable, wail, wallow, wane, waning, wanton, war-like, warily, wariness, warlike, warned, warning, warp, warped, wary, washed-out, waste, wasted, wasteful, wastefulness, wasting, water-down, watered-down, wayward, weak, weaken, weakening, weaker, weakness, weaknesses, weariness, wearisome, weary, wedge, weed, weep, weird, weirdly, wheedle, whimper, whine, whining, whiny, whips, wicked, wickedly, wickedness, wild, wildly, wiles, wilt, wily, wimpy, wince, wobble, wobbled, wobbles, woe, woebegone, woeful, woefully, womanizer, womanizing, worn, worried, worriedly, worrier, worries, worrisome, worry, worrying, worryingly, worse, worsen, worsening, worst, worthless, worthlessly, worthlessness, wound, wounds, wrangle, wrath, wreak, wreaked, wreaks, wreck, wrest, wrestle, wretch, wretched, wretchedly, wretchedness, wrinkle, wrinkled, wrinkles, writhe, wrong, wrongful, wrongly, wrought, yawn, zap, zapped, zaps, zealot, zealous, zealously, zombie\"\n",
    "# neg_words = word_tokenize(neg_words)\n",
    "# # remove all instances of \",\"\n",
    "# neg_words = [word for word in neg_words if word != \",\"]\n",
    "# neg_words = [w for w in neg_words if not w in stop_words]\n",
    "# print(\"There are %s negative words\" % len(neg_words))\n",
    "# all_neg_words = [w for w in neg_words if w in speech_string_2010]\n",
    "# print(len(all_neg_words))\n",
    "# #print(all_neg_words)\n",
    "# freq_neg_words = nltk.FreqDist(all_neg_words)\n",
    "# print(len(freq_neg_words))\n",
    "# print(freq_neg_words.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2nd neg words dataset\n",
    "neg_words = \"abysmal,adverse,alarming,angry,annoy,anxious,apathy,appalling,atrocious,awful,bad,banal,barbed,belligerent,bemoan,beneath,boring,broken,callous,can't,cannot,clumsy,coarse,cold,collapse,confused,contradictory,contrary,corrosive,corrupt,crazy,creepy,criminal,cruel,cry,cutting,damage,damaging,dastardly,dead,decaying,deformed,deny,deplorable,depressed,deprived,despicable,detrimental,dirty,disease,disgusting,disheveled,dishonest,dishonorable,dismal,distress,don't,dreadful,dreary,enraged,eroding,evil,fail,faulty,fear,feeble,fight,filthy,foul,frighten,frightful,gawky,ghastly,grave,greed,grim,grimace,gross,grotesque,gruesome,guilty,haggard,hard,harmful,hate,hideous,horrendous,horrible,hostile,hurt,hurtful,icky,ignorant,ignore,ill,immature,imperfect,impossible,inane,inelegant,infernal,injure,injurious,insane,insidious,insipid,jealous,junky,lose,lousy,lumpy,malicious,mean,menacing,messy,misshapen,missing,misunderstood,moan,moldy,monstrous,naive,nasty,naughty,negate,negative,never,nonobody,nondescript,nonsense,not,noxious,objectionable,odious,offensive,old,oppressive,pain,perturb,pessimistic,petty,plain,poisonous,poor,prejudice,questionable,quirky,quit,reject,renege,repellant,reptilian,repugnant,repulsive,revenge,revolting,rocky,rotten,rude,ruthless,sad,savage,scare,scary,scream,severe,shocking,shoddy,sick,sickening,sinister,slimy,smelly,sobbing,sorry,spiteful,sticky,stinky,stormy,stressful,stuck,stupid,substandard,suspect,suspicious,tense,terrible,terrifying,threatening,ugly,undermine,unfair,unfavorable,unhappy,unhealthy,unjust,unlucky,unpleasant,unsatisfactory,unsightly,untoward,unwanted,unwelcome,unwholesome,unwieldy,unwise,upset,vice,vicious,vile,villainous,vindictive,wary,weary,wicked,woeful,worthless,wound,yell,yucky,zero\"\n",
    "neg_words = word_tokenize(neg_words)\n",
    "\n",
    "# remove all instances of \",\"\n",
    "neg_words = [word for word in neg_words if word != \",\"]\n",
    "neg_words = [w for w in neg_words if not w in stop_words]\n",
    "#print(\"There are %s negative words\" % len(neg_words))\n",
    "\n",
    "all_neg_words = [w for w in all_words if w in neg_words]\n",
    "\n",
    "freq_neg_words = nltk.FreqDist(all_neg_words)\n",
    "\n",
    "top_50_neg_words = [word[0] for word in freq_neg_words.most_common(50)]\n",
    "\n",
    "top_50_neg_words_string = \"\"\n",
    "\n",
    "for word in top_50_neg_words:\n",
    "    top_50_neg_words_string += (word + \" \")\n",
    "    \n",
    "\n",
    "# Create the wordcloud object\n",
    "wordcloud = WordCloud(width=960, height=960).generate(top_50_neg_words_string)\n",
    " \n",
    "# Display the generated image:\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.margins(x=0, y=0)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "all_neg_words_1997 = [w for w in neg_words if w in speech_string_1997]\n",
    "print(len(all_neg_words_1997))\n",
    "all_neg_words_1998 = [w for w in neg_words if w in speech_string_1998]\n",
    "print(len(all_neg_words_1998))\n",
    "all_neg_words_1999 = [w for w in neg_words if w in speech_string_1999]\n",
    "print(len(all_neg_words_1999))\n",
    "all_neg_words_2000 = [w for w in neg_words if w in speech_string_2000]\n",
    "print(len(all_neg_words_2000))\n",
    "all_neg_words_2001 = [w for w in neg_words if w in speech_string_2001]\n",
    "print(len(all_neg_words_2001))\n",
    "all_neg_words_2002 = [w for w in neg_words if w in speech_string_2002]\n",
    "print(len(all_neg_words_2002))\n",
    "all_neg_words_2003 = [w for w in neg_words if w in speech_string_2003]\n",
    "print(len(all_neg_words_2003))\n",
    "all_neg_words_2004 = [w for w in neg_words if w in speech_string_2004]\n",
    "print(len(all_neg_words_2004))\n",
    "all_neg_words_2005 = [w for w in neg_words if w in speech_string_2005]\n",
    "print(len(all_neg_words_2005))\n",
    "all_neg_words_2009 = [w for w in neg_words if w in speech_string_2009]\n",
    "print(len(all_neg_words_2009))\n",
    "all_neg_words_2010 = [w for w in neg_words if w in speech_string_2010]\n",
    "print(len(all_neg_words_2010))\n",
    "all_neg_words_2011 = [w for w in neg_words if w in speech_string_2011]\n",
    "print(len(all_neg_words_2011))\n",
    "all_neg_words_2012 = [w for w in neg_words if w in speech_string_2012]\n",
    "print(len(all_neg_words_2012))\n",
    "all_neg_words_2014 = [w for w in neg_words if w in speech_string_2014]\n",
    "print(len(all_neg_words_2014))\n",
    "all_neg_words_2015 = [w for w in neg_words if w in speech_string_2015]\n",
    "print(len(all_neg_words_2015))\n",
    "all_neg_words_2016 = [w for w in neg_words if w in speech_string_2016]\n",
    "print(len(all_neg_words_2016))\n",
    "all_neg_words_2017 = [w for w in neg_words if w in speech_string_2017]\n",
    "print(len(all_neg_words_2017))\n",
    "all_neg_words_2018 = [w for w in neg_words if w in speech_string_2018]\n",
    "print(len(all_neg_words_2018))\n",
    "\n",
    "#print(all_neg_words)\n",
    "# freq_neg_words = nltk.FreqDist(all_neg_words)\n",
    "# print(len(freq_neg_words))\n",
    "# print(freq_neg_words.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "year = [ \"98\", \"99\", \"00\", \"01\", \"02\", \"03\", \"04\", \"05\", \"09\", \"10\", \"11\", \"12\", \"14\", \"15\", \"16\", \"17\", \"18\"]\n",
    "total_pos = [154,146,151,153,136,138,132,171,122,136,131,144,139,153,146,150,141]\n",
    "total_neg = [84,71,64,81,60,66,57,80,58,54,69,50,69,77,70,69,67]\n",
    "#pos_neg = [[63,25],[154,84],[146,71],[151,64],[153,81],[136,60],[138,66],[132,57],[171,80],[122,58],[136,54],[131,69],[144,50],[139,69],[153,77],[146,70],[150,69]]\n",
    "\n",
    "xpos = np.arange(len(total_pos))\n",
    "\n",
    "plt.yticks(xpos, year)\n",
    "plt.xlabel(\"No. of words\")\n",
    "plt.ylabel(\"Year\")\n",
    "plt.barh(xpos, total_pos, label = \"pos\")\n",
    "plt.barh(xpos, total_neg,  label = \"neg\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create speech string for each individual Taoiseach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.taoiseach.gov.ie/eng/News/Taoiseach's_Speeches/?pageNumber=\"\n",
    "\n",
    "count = 1\n",
    "\n",
    "for page in range(1,38):\n",
    "    r = requests.get(base_url + str(page))\n",
    "    c = r.content\n",
    "    soup = BeautifulSoup(c, \"html.parser\")\n",
    "    all = soup.find_all(\"span\",{\"class\":\"ItemName\"})\n",
    "    for a in all:\n",
    "        r = requests.get(\"https://www.taoiseach.gov.ie\" + a.find(\"a\")[\"href\"])\n",
    "        c = r.content\n",
    "        \n",
    "        soup = BeautifulSoup(c, \"html.parser\")\n",
    "        all = soup.find(\"div\", {\"class\":\"contentSub\"}).text\n",
    "        all = all.replace(\"var mapOverlayUrl = '';\",\"\")\n",
    "        \n",
    "        if count < 92:\n",
    "            leo_string += all\n",
    "            item = (word_tokenize(all), \"fg\")\n",
    "            speech_list.append(item)\n",
    "        elif count < 369:\n",
    "            enda_string += all\n",
    "            item = (word_tokenize(all), \"fg\")\n",
    "            speech_list.append(item)\n",
    "        count = count + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(speech_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(speech_list)\n",
    "all_words = word_tokenize(all_speeches_string)\n",
    "#print(len(all_words)) #1667544\n",
    "#print((all_words[:100]))\n",
    "#print(speech_list[0])\n",
    "print(type(speech_list[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(all_words)\n",
    "common_words =[]\n",
    "for i in all_words.most_common(3000):\n",
    "    common_words.append(i[0])\n",
    "print(len(common_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(document):\n",
    "    unique_words_in_document = set(document)\n",
    "    features = {}\n",
    "    for w in common_words:\n",
    "        features[w] = (w in unique_words_in_document)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find_features(speech_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "featuresets = []\n",
    "for(review_words, review_category) in speech_list:\n",
    "    featuresets.append((find_features(review_words),review_category))\n",
    "print(len(featuresets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_set = featuresets[:500]\n",
    "test_set = featuresets[500:]\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "accuracy = nltk.classify.accuracy(classifier, test_set)\n",
    "print(\"Accuracy = \", accuracy * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "training_set = featuresets[:189]\n",
    "test_set = featuresets[189:]\n",
    "\n",
    "#Train our classifier using the training set\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "#Save the classifier\n",
    "save_classifier = open('naivebayes.pickle','wb')\n",
    "pickle.dump(classifier, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the classifier\n",
    "classifier_f = open('naivebayes.pickle','rb')\n",
    "classifier = pickle.load(classifier_f)\n",
    "classifier_f.close()\n",
    "\n",
    "#Classify using out test set with our classifier, calculate the accuracy\n",
    "accuracy = nltk.classify.accuracy(classifier, test_set)\n",
    "\n",
    "print('NaÃ¯ve Bayes Classifier Accuracy = ', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
